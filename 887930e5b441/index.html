<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>强化学习数学原理笔记 | Little_sk</title><meta name="author" content="Little_sk"><meta name="copyright" content="Little_sk"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="可能会出现公式加载错误的情况，可以直接下载[pdf版本](https:&#x2F;&#x2F;lsk404.github.io&#x2F;files&#x2F;RLnote.pdf)。 强化学习数学原理第一章基本概念 grid-world example ： 一个机器人走网格的经典例子，机器人尽量避免进入forbidden grid、尽量减少拐弯、不要走出边界、……  state: 状态，表示为一个节点，在grid-world中可以表示">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习数学原理笔记">
<meta property="og:url" content="https://lsk404.github.io/887930e5b441/index.html">
<meta property="og:site_name" content="Little_sk">
<meta property="og:description" content="可能会出现公式加载错误的情况，可以直接下载[pdf版本](https:&#x2F;&#x2F;lsk404.github.io&#x2F;files&#x2F;RLnote.pdf)。 强化学习数学原理第一章基本概念 grid-world example ： 一个机器人走网格的经典例子，机器人尽量避免进入forbidden grid、尽量减少拐弯、不要走出边界、……  state: 状态，表示为一个节点，在grid-world中可以表示">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://lsk404.github.io/img/avator.jpg">
<meta property="article:published_time" content="2025-04-16T09:53:00.000Z">
<meta property="article:modified_time" content="2025-06-07T09:37:07.349Z">
<meta property="article:author" content="Little_sk">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="数学">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lsk404.github.io/img/avator.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "强化学习数学原理笔记",
  "url": "https://lsk404.github.io/887930e5b441/",
  "image": "https://lsk404.github.io/img/avator.jpg",
  "datePublished": "2025-04-16T09:53:00.000Z",
  "dateModified": "2025-06-07T09:37:07.349Z",
  "author": [
    {
      "@type": "Person",
      "name": "Little_sk",
      "url": "https://lsk404.github.io/lsk404.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="https://lsk404.github.io/887930e5b441/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-745T2DDK98"></script><script>window.dataLayer = window.dataLayer || []
function gtag(){dataLayer.push(arguments)}
gtag('js', new Date())
gtag('config', 'G-745T2DDK98')
btf.addGlobalFn('pjaxComplete', () => {
  gtag('config', 'G-745T2DDK98', {'page_path': window.location.pathname})
}, 'google_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"No results found for: ${query}","hits_stats":"${hits} articles found"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":150,"languages":{"author":"Author: Little_sk","link":"Link: ","source":"Source: Little_sk","info":"Copyright belongs to the author. For commercial use, please contact the author for authorization. For non-commercial use, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '强化学习数学原理笔记',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(img/bg3.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avator.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">51</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间线</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/bg.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/favicon.ico" alt="Logo"></a><a class="nav-page-title" href="/"><span class="site-name">强化学习数学原理笔记</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间线</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">强化学习数学原理笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-04-16T09:53:00.000Z" title="Created 2025-04-16 17:53:00">2025-04-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-06-07T09:37:07.349Z" title="Updated 2025-06-07 17:37:07">2025-06-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">342</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>可能会出现公式加载错误的情况，可以直接下载[pdf版本](<a href="https://lsk404.github.io/files/RLnote.pdf)。">https://lsk404.github.io/files/RLnote.pdf)。</a></p>
<h1 id="强化学习数学原理"><a href="#强化学习数学原理" class="headerlink" title="强化学习数学原理"></a>强化学习数学原理</h1><h2 id="第一章基本概念"><a href="#第一章基本概念" class="headerlink" title="第一章基本概念"></a>第一章基本概念</h2><ul>
<li><p>grid-world example ： 一个机器人走网格的经典例子，机器人尽量避免进入forbidden grid、尽量减少拐弯、不要走出边界、……</p>
</li>
<li><p>state: 状态，表示为一个节点，在grid-world中可以表示为一个格子（也可以添加其他信息到状态，如速度等）</p>
</li>
<li><p>state space：状态空间，所有状态的集合。</p>
</li>
<li><p>action：行动，能够使得状态变化的动作。（如向上/下/左/右移动，等）</p>
</li>
<li><p>action space：行动的集合，通常依赖于当前的状态。</p>
</li>
<li><p>state transition：状态转移，从一个状态移动到另一个状态。</p>
<p>  <script type="math/tex">s_5 \overset{a_1}{\rightarrow} s_6</script> 表示从状态<script type="math/tex">s_5</script> 经过动作<script type="math/tex">a_1</script> 到达状态 <script type="math/tex">a_6</script></p>
</li>
<li><p>state transition probability: 状态转移的条件概率。（例如：<script type="math/tex">p(s_2|s_1,a_2) = 0.8</script> 代表在状态<script type="math/tex">s_1</script>，行动<script type="math/tex">a_2</script> 下，<script type="math/tex">s_2</script>的概率是0.8)</p>
</li>
<li><p>Policy: 策略，用箭头来表示。表示在某个状态更倾向于走哪个action</p>
<p>  undefinedpi(a_1|s_1)=0 ,\pi(a_2|s_1)=1,\pi(a_3|s_1)=0 ,\pi(a_4|s_1)=0<script type="math/tex">表示在状态</script>s_1<script type="math/tex">有1的概率进行行动</script>a_2<script type="math/tex">。显然</script>\sum_{i=1}^k \pi(a_i|s_1) = 1$</p>
</li>
<li><p>reward: 他是一个实数，代表我们的奖励，如果<script type="math/tex">reward>0</script>,则代表希望它发生，<script type="math/tex">reward<0</script>则表示不希望它发生。</p>
<p>  例如我们可以将“尝试逃出边界的时候，我们设<script type="math/tex">r_{bound} = -1</script> , 将到达目的地设为<script type="math/tex">r_{target} = 1</script></p>
<p>  因此我们可以通过设计reward来实现到达目的地。</p>
<p>  <script type="math/tex">p(r=-1|s_1,a_1) = 1, p(r \not= -1 |s_1,a_1)=0</script> 表示在状态<script type="math/tex">s_1</script> 进行<script type="math/tex">a_1</script> 得到-1的reward的概率是1，得到不是-1的reward的概率是0</p>
</li>
<li><p>trajectory：一个由state、action、reward连接成的链。</p>
</li>
<li><p>return：一个trajectory中所有的reward的总和。通过比较return来评估策略是好是坏</p>
</li>
<li><p>Discounted rate : undefinedgamma \in [0,1)<script type="math/tex">。</script>discounted return = r_0 + \gamma r_1 + \gamma ^2 r_2 + …$ ,</p>
<p>  undefinedgamma<script type="math/tex">通常表示是否更看重未来，</script>\gamma$ 越小，则越看重现在。</p>
</li>
<li><p>Episode: 能够到达terminal states(停止状态) 的trajectory。一个Episode也叫一个Episode task与之对应的是continuing task（指永无止境的任务）。</p>
</li>
</ul>
<h3 id="Markov-decision-process（MDP）"><a href="#Markov-decision-process（MDP）" class="headerlink" title="Markov decision process（MDP）"></a>Markov decision process（MDP）</h3><ul>
<li><p>集合：</p>
<ul>
<li><p>State：状态集合</p>
</li>
<li><p>Action：对于每个状态s的行动集合<script type="math/tex">A(s)</script></p>
</li>
<li><p>Reward：奖励集合<script type="math/tex">R(s,a)</script></p>
</li>
</ul>
</li>
<li><p>概率要素(probability distribution)：</p>
<ul>
<li><p>State transition probability:<script type="math/tex">p(s'|s,a)</script> 在状态s下，进行行动a，到达另一个状态<script type="math/tex">s'</script>的概率。</p>
</li>
<li><p>Reward probability: <script type="math/tex">p(r|s,a)</script>在状态s下，进行行动a，得到r的奖励的概率。</p>
</li>
<li><p>Policy: undefinedpi(a|s)$ 在状态s下，进行行动a的概率。</p>
</li>
</ul>
</li>
</ul>
<p>MDP的独特性质（markov property）：与历史无关</p>
<p><script type="math/tex">p(s_{t+1} |a_{t+1}s_t...a_1s_0) = p(s_{t+1}|a_{t+1},s_t)</script> </p>
<p><script type="math/tex">p(r_{t+1} |a_{t+1}s_t...a_1s_0) = p(r_{t+1}|a_{t+1},s_t)</script> </p>
<h2 id="第二章-贝尔曼公式"><a href="#第二章-贝尔曼公式" class="headerlink" title="第二章 贝尔曼公式"></a>第二章 贝尔曼公式</h2><h3 id="return"><a href="#return" class="headerlink" title="return"></a>return</h3><p>为什么return很重要？因为return评估的策略的好坏。</p>
<p>如何计算return？用<script type="math/tex">v_i</script> 表示从<script type="math/tex">s_i</script> 出发得到的return。</p>
<p>以下面的状态图为例： </p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lsk404.github.io/files/posts_images/RLnote2_1.png" alt="image-20250321154315912" style="zoom:50%;" /></p>
<ul>
<li><p>那么根据定义有</p>
<p>  <script type="math/tex">v_1 = r_1 + \gamma r_2 + \gamma^2 r_3 + ...</script></p>
<p>  <script type="math/tex">v_2 = r_2 + \gamma r_3 + \gamma^2 r_4 + ...</script></p>
<p>  <script type="math/tex">v_3 = r_3 + \gamma r_4 + \gamma^2 r_1 + ...</script></p>
<p>  <script type="math/tex">v_4 = r_4 + \gamma r_1 + \gamma^2 r_2 + ...</script></p>
</li>
<li><p>也可以递推得到(Booststrapping):一个状态依赖于其他状态</p>
<p>  <script type="math/tex">v_1 = r_1 + \gamma(r_2 + \gamma r_3 + ...) = r_1 + \gamma v_2</script></p>
<p>  <script type="math/tex">v_2 = r_2 + \gamma(r_3 + \gamma r_4 + ...) = r_2 + \gamma v_3</script></p>
<p>  <script type="math/tex">v_3 = r_3 + \gamma(r_4 + \gamma r_1 + ...) = r_3 + \gamma v_4</script></p>
<p>  <script type="math/tex">v_4 = r_4 + \gamma(r_1 + \gamma r_2 + ...) = r_4 + \gamma v_1</script></p>
<p>  然后就有<script type="math/tex">v = r + \gamma P*v</script> ，这里的<script type="math/tex">v,r</script> 是向量，<script type="math/tex">P</script>是变换矩阵。于是就能求解得出v的值。</p>
</li>
</ul>
<h3 id="state-value"><a href="#state-value" class="headerlink" title="state value"></a>state value</h3><p><script type="math/tex">S_t \overset{A_t}{\to} R_{t+1},S_{t+1}</script> ,指在<script type="math/tex">S_t</script> 下经过<script type="math/tex">A_t</script> 得到undefinedR_{t+1},S_{t+1})$</p>
<p>**state value：** <script type="math/tex">v_\pi(s) = \mathbb{E}[G_t|S_t=s]</script>当前状态为s的时候所有的return的期望值。</p>
<p>考虑下面的trajectory:<script type="math/tex">S_t \overset{A_t}{\to} R_{t+1},S_{t+1}\overset{A_{t+1}}{\to} R_{t+2},S_{t+2}\overset{A_{t+2}}{\to} R_{t+3},S_{t+3}...</script></p>
<p>那么<script type="math/tex">G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = R_{t+1} + \gamma G_{t+1}</script></p>
<p>则有</p>
<p><script type="math/tex">v_\pi(s) = \mathbb{E}[G_t|S_t=s] \\ = \mathbb{E}[R_t|S_t = s] + \mathbb{E}[G_{t+1}|S_t = s]</script> </p>
<p>公式中的第一项</p>
<p>undefinedmathbb{E}[R_{t+1}|S_t = s] = \underset{a}{\sum} [\pi(a|s) \underset{r}{\sum}p(r|s,a)r]$</p>
<p>公式中的第二项</p>
<script type="math/tex; mode=display">
\mathbb{E}[G_{t+1}|S_t = s] = \underset{s'}{\sum}\{\mathbb{E}[G_{t+1}|S_t=s,S_{t+1}=s']p(s'|s) \}
\\ =\underset{s'}{\sum}\mathbb{E}[G_{t+1}|S_{t+1}=s']p(s'|s)
\\ = \underset{s'}{\sum}\{v_\pi(s')p(s'|s)\} 
\\ = \underset{s'}{\sum}\{ v_\pi(s')\underset{a}{\sum}[p(s'|s,a)\pi(a|s)]\}</script><ul>
<li>于是，贝尔曼公式：</li>
</ul>
<script type="math/tex; mode=display">
v_\pi(s) = \mathbb{E}[R_t|S_t = s] + \mathbb{E}[G_{t+1}|S_t = s]
\\ 
=\underset{a}{\sum} \pi(a|s) [\underset{r}{\sum}p(r|s,a)r+\gamma\underset{s'}{\sum}p(s'|s,a)v_\pi(s')]</script><ul>
<li>之后我们列出每个<script type="math/tex">s_i</script>对应的<script type="math/tex">v_\pi(s_i)</script>的公式，然后求解方程组即可得到每个状态的state value。</li>
</ul>
<h3 id="贝尔曼公式的矩阵-向量形式"><a href="#贝尔曼公式的矩阵-向量形式" class="headerlink" title="贝尔曼公式的矩阵/向量形式"></a>贝尔曼公式的矩阵/向量形式</h3><ul>
<li><p>由贝尔曼公式：<script type="math/tex">v_\pi(s)=\underset{a}{\sum} \pi(a|s) [\underset{r}{\sum}p(r|s,a)r+\gamma\underset{s'}{\sum}p(s'|s,a)v_\pi(s')]</script></p>
<p>  可以简略写为<script type="math/tex">v_\pi(s) = r_\pi(s) + \gamma \underset{s'}{\sum}p_\pi(s'|s)v_\pi(s')</script> , (这里有<script type="math/tex">r_\pi(s) \iff \underset{a}{\sum} \pi(a|s)\underset{r}{\sum}p(r|s,a)r \\ p_\pi(s'|s) \iff \underset{a}{\sum} \pi(a|s)p(s'|s,a)</script> )</p>
<p>  ​    对s进行标号得出<script type="math/tex">v_\pi(s_i) = r_\pi(s_i) + \gamma \underset{s_j}{\sum}p_\pi(s_j|s_i)v_\pi(s_j)</script></p>
<p>  于是化为矩阵向量形式：<script type="math/tex">v_\pi = r_\pi + \gamma P_\pi v_\pi</script> ,这里的<script type="math/tex">v_\pi,r_\pi</script> 均为向量，<script type="math/tex">P_\pi</script> 为状态转移矩阵(<script type="math/tex">[P_\pi]_{i,j} = p_\pi(s_j | s_i)</script> )</p>
</li>
<li><p>求解贝尔曼公式，进而得到state value是评判策略好坏(policy evaluation)的关键。</p>
</li>
<li><p>求解贝尔曼公式：</p>
<ul>
<li><p><script type="math/tex">v_\pi = (I - \gamma P_\pi)^{-1}r_{\pi}</script> ,很简洁的公式，但是求逆太难算了。</p>
</li>
<li><p>迭代求法：<script type="math/tex">v_{k+1} = r_\pi + \gamma P_\pi v_k</script>  </p>
<p>  当<script type="math/tex">k \to \infty</script>时 ,有<script type="math/tex">v_k = v_\pi</script> </p>
</li>
</ul>
</li>
</ul>
<h3 id="action-value"><a href="#action-value" class="headerlink" title="action value"></a>action value</h3><p>从一个状态出发， 选择了一个action之后， 得到的return的期望。</p>
<p>在做判断时，根据Action value的大小来判断选择哪个Action。</p>
<p>求解状态s下进行行动a的action value(<script type="math/tex">q_{\pi}(s,a)</script>)</p>
<script type="math/tex; mode=display">
q_\pi(s,a) = \underset{r}{\sum}p(r|s,a)r+\gamma\underset{s'}{\sum}p(s'|s,a)v_\pi(s')</script><p>计算action value：</p>
<ol>
<li>根据state value求出</li>
<li>直接计算action value</li>
</ol>
<h2 id="第三章-贝尔曼最优公式"><a href="#第三章-贝尔曼最优公式" class="headerlink" title="第三章 贝尔曼最优公式"></a>第三章 贝尔曼最优公式</h2><ul>
<li>直观上地说，选择action value比较大的action，将他设置为1，其他设置为0。不断如此迭代，就可以找到最优的策略。</li>
<li>严格证明则需要贝尔曼最优公式。</li>
</ul>
<p>如果对于所有的s，都有<script type="math/tex">v_{\pi_1}(s) \ge v_{\pi_2}(s) for\ all \ s \in S</script> ,那么说undefinedpi_1<script type="math/tex">是比</script>\pi_2$要好的。</p>
<ol>
<li>问题一：这样最优的策略是否存在？</li>
<li>问题二：这个最优的策略是唯一的吗？</li>
<li>问题三：策略是stochastic还是deterministic？</li>
<li>问题四：如何找到这么一个策略？</li>
</ol>
<h3 id="贝尔曼最优公式"><a href="#贝尔曼最优公式" class="headerlink" title="贝尔曼最优公式"></a>贝尔曼最优公式</h3><ul>
<li>贝尔曼最优公式堂堂登场！</li>
</ul>
<script type="math/tex; mode=display">
v(s) = max(\underset{a}{\sum} (\pi(a|s) q(s,a))) , s \in S</script><p> 可以发现，假设当<script type="math/tex">a = a'</script>时，<script type="math/tex">q(s,a)</script> 最大，那么令undefinedpi(a|s) = \begin{equation}   \left\{               \begin{array}{**lr**}               1 &amp;a = a’  \\               0 &amp; a\not= a’                \end{array}   \right.   \end{equation} <script type="math/tex">,就会得到最大的</script>v(s)$。</p>
<p>即<script type="math/tex">v(s) = max(\underset{a}{\sum} (\pi(a|s) q(s,a)))=\underset{a\in A(s)}{max}(q(s,a))</script></p>
<ul>
<li>将公式变为矩阵-向量形式</li>
</ul>
<script type="math/tex; mode=display">
v = \underset{\pi}{max}(r_\pi + \gamma P_\pi v)</script><p>我们设一个映射<script type="math/tex">f(v) := \underset{\pi}{max}(r_\pi + \gamma P_\pi v)</script> ,那么原式就可以化为<script type="math/tex">v=f(v)</script> </p>
<h3 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h3><p>FixedPoint 不动点： 对于映射<script type="math/tex">f : X \to X</script> ，存在<script type="math/tex">x \in X, f(x) = x</script> ，那么x是不动点。</p>
<p>Contraction mapping : 在映射后两点的距离更小。<script type="math/tex">||f(x_1)-f(x_2)|| = \gamma||x_1 - x_2|| ,\gamma < 1</script> 。(例如<script type="math/tex">f(x) = 0.5x</script> 就是一个contraction mapping)</p>
<h3 id="contraction-Theorem"><a href="#contraction-Theorem" class="headerlink" title="contraction Theorem"></a>contraction Theorem</h3><p>如果<script type="math/tex">f</script>是一个contraction mapping。那么一定有</p>
<ol>
<li>存在一个<script type="math/tex">x*</script> , 满足<script type="math/tex">f(x^*) = x^*</script> ，即<script type="math/tex">x^*</script> 是一个FixedPoint</li>
<li>这样的<script type="math/tex">x^*</script> 一定有且只有一个</li>
<li>可以通过迭代算法求出这个<script type="math/tex">x^*</script> : <script type="math/tex">x_{k+1} = f(x_k)</script> ，当<script type="math/tex">k \to \infty</script> 时，有<script type="math/tex">x_k \to x^*</script></li>
</ol>
<p>例如<script type="math/tex">f(x) = 0.5x</script> ,那么<script type="math/tex">f(0) = 0, x^* = 0</script> ,给出任意x，在不断进行<script type="math/tex">x = 0.5x</script> 迭代后，会收敛于<script type="math/tex">0</script></p>
<h3 id="求解贝尔曼最优公式"><a href="#求解贝尔曼最优公式" class="headerlink" title="求解贝尔曼最优公式"></a>求解贝尔曼最优公式</h3><p>可以证明在贝尔曼最优公式中<script type="math/tex">f(v) = \underset{\pi}{max}(r_\pi + \gamma P_\pi v)</script> 是一个contraction mapping，那么<script type="math/tex">v = f(v)</script> 。于是就可以通过迭代算法来求解出来。</p>
<p>假设<script type="math/tex">v^*</script> 是贝尔曼最优公式的解，即是他的不动点。即<script type="math/tex">v^* = \underset{\pi}{max}(r_\pi + \gamma P_\pi v^*)</script>  </p>
<p>所以就可以利用contraction Theorem中的迭代算法来求得<script type="math/tex">v^*</script></p>
<p>所以**贝尔曼最优公式就是特殊的贝尔曼公式**。</p>
<h2 id="第四章-Value-iteration-amp-Policy-iteration"><a href="#第四章-Value-iteration-amp-Policy-iteration" class="headerlink" title="第四章 Value iteration &amp; Policy iteration"></a>第四章 Value iteration &amp; Policy iteration</h2><h3 id="Value-iteration-algorithm-值迭代算法"><a href="#Value-iteration-algorithm-值迭代算法" class="headerlink" title="Value iteration algorithm(值迭代算法)"></a>Value iteration algorithm(值迭代算法)</h3><p>值迭代算法就是根据贝尔曼最优公式来迭代求解优化问题。</p>
<p><script type="math/tex">v_{k+1} = f(v_k) = \underset{\pi}{max}(r_\pi + \gamma P_\pi v_\pi)</script></p>
<hr>
<p>**求解步骤：**最开始生成一个任意的状态<script type="math/tex">v_0</script> ,不断循环以下两步</p>
<ol>
<li>policy update更新策略：undefinedpi_{k+1} = \underset{\pi}{argmax}(r_\pi + \gamma P_\pi v_k)$</li>
<li>value update 更新值: <script type="math/tex">v_{k+1} = r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}} v_k</script></li>
</ol>
<p>需要注意的是<script type="math/tex">v_k</script> 只是一个值，并不是一个state value。</p>
<p>不断迭代直到<script type="math/tex">v_k-v_{k-1}</script> 足够小就认为已经收敛了。</p>
<h3 id="Policy-iteration-algorithm-策略迭代算法"><a href="#Policy-iteration-algorithm-策略迭代算法" class="headerlink" title="Policy iteration algorithm(策略迭代算法)"></a>Policy iteration algorithm(策略迭代算法)</h3><p>最开始生成一个任意策略undefinedpi_0$</p>
<ol>
<li>policy evalution(PE): <script type="math/tex">v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}</script></li>
<li>policy improvement(PI): undefinedpi_{k+1}=\underset{\pi}{argmax}(r_\pi + \gamma P_\pi v_{\pi_k})$ </li>
</ol>
<p>整体过程即undefinedpi_0 \overset{PE}{\to}v_{\pi_0} \overset{PI}{\to}\pi_1\overset{PE}{\to}v_{\pi_1} \overset{PI}{\to}\pi_2\overset{PE}{\to}v_{\pi_2} \overset{PI}{\to}\pi_3….$</p>
<ul>
<li><p>几个核心问题：</p>
<ol>
<li><p>在policy evaluation中如何求解 state value？</p>
</li>
<li><p>为什么进行PI后， undefinedpi_{k+1}<script type="math/tex">比</script>\pi_k$  更优？</p>
</li>
<li><p>为什么最终能找到最优解？</p>
</li>
<li><p>Policy iteration和Value iteration 有什么关系？</p>
</li>
</ol>
</li>
<li><p>Q1: 有两种方法(即求解贝尔曼公式的两种方法)：</p>
<ol>
<li>closed-form solution : <script type="math/tex">v_{\pi_k} = (I-\gamma P_{\pi_k})^{-1} r_{\pi_k}</script></li>
<li><p>iterative solution: <script type="math/tex">v^{j+1}_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}^{(j)} , j = 0,1,2,...</script></p>
<p>Q2: undefinedpi_{k+1}=\underset{\pi}{argmax}(r_\pi + \gamma P_\pi v_{\pi_k})<script type="math/tex">，因为</script>\pi_{k+1}<script type="math/tex">一定比</script>\pi_k$ 要更大</p>
<p>Q3：<script type="math/tex">v_{\pi_0} \le v_{\pi_1} \le v_{\pi_2} \le ...\le v_{\pi_k} \le v^*</script></p>
<p>Q4: 二者是两个极端</p>
</li>
</ol>
</li>
</ul>
<h3 id="truncated-policy-iteration-algorithm"><a href="#truncated-policy-iteration-algorithm" class="headerlink" title="truncated policy iteration algorithm"></a>truncated policy iteration algorithm</h3><p>他是值迭代算法和策略迭代算法的推广，值迭代算法和策略迭代算法是truncated policy iteration algorithm的极端情况。</p>
<p>Policy iteration: undefinedpi_0 \overset{PE}{\to}v_{\pi_0} \overset{PI}{\to}\pi_1\overset{PE}{\to}v_{\pi_1} \overset{PI}{\to}\pi_2\overset{PE}{\to}v_{\pi_2} \overset{PI}{\to}\pi_3….$</p>
<p>Value iteration:         <script type="math/tex">u_0\overset{PU}{\to}\pi_1'\overset{VU}{\to}u_1\overset{PU}{\to}\pi_2'\overset{VU}{\to}u_2...</script></p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Policy Iteration algorithm</th>
<th>Value iteration algorithm</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>1）Policy：</td>
<td>undefinedpi _0$</td>
<td>N/A</td>
<td></td>
</tr>
<tr>
<td>2) Value:</td>
<td><script type="math/tex">v_{\pi_0} = r_{\pi_0}+\gamma P_{\pi_0} v_{\pi_0}</script></td>
<td><script type="math/tex">v_0 := v_{\pi_0}</script></td>
<td></td>
</tr>
<tr>
<td>3) Policy:</td>
<td>undefinedpi_1 = \underset{\pi}{argmax}(r_\pi + \gamma P_\pi v_{\pi_0})$</td>
<td>undefinedpi_1 = \underset{\pi}{argmax}(r_\pi + \gamma P_\pi v_0)$</td>
<td>两个算法的第一步Policy是相同的。</td>
</tr>
<tr>
<td>4) Value:</td>
<td><script type="math/tex">v_{\pi_1} = r_{\pi_1} + \gamma P_{\pi_1} \textcolor{ #FF0000}{v_{\pi_1}}</script></td>
<td><script type="math/tex">v_1 = r_{\pi_1} + \gamma P_{\pi_1}\textcolor{ #FF0000}{v_0}</script></td>
<td>两个算法求<script type="math/tex">v_\pi</script> 的方法是不一样的</td>
</tr>
<tr>
<td>5）Policy：</td>
<td>undefinedpi_2 = \underset{\pi}{argmax}(r_\pi + \gamma P_\pi v_{\pi_1})$</td>
<td>undefinedpi_2’ = \underset{\pi}{argmax}(r_\pi + \gamma P_\pi v_1)$</td>
<td></td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
</div>
<p>区别在于计算<script type="math/tex">v_{\pi_1}</script>的时候是使用贝尔曼公式求，还是直接继承上一步的求法。</p>
<p>考虑公式<script type="math/tex">v_{\pi_1} = r_{\pi_1} + \gamma P_{\pi_1} v _{\pi_1}</script> </p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{\pi_1}^{(0)} &= v_0\\
v_{\pi_1}^{(1)} &= r_{\pi_1} + \gamma P_{\pi_1} v _{\pi_1}^{(0)} & \to  value\ iteration\ \textcolor{ #FF0000}{v_1}\\
v_{\pi_1}^{(2)} &= r_{\pi_1} + \gamma P_{\pi_1} v _{\pi_1}^{(1)} \\
...\\
v_{\pi_1}^{(j)} &= r_{\pi_1} + \gamma P_{\pi_1} v _{\pi_1}^{(j-1)} & \to truncated\ policy\ iteration\  \textcolor{ #FF0000}{\overline v_1}\\
...\\
v_{\pi_1}^{(\infty)} &= r_{\pi_1} + \gamma P_{\pi_1} v _{\pi_1}^{(\infty)} &\to policy\ iteration\ \textcolor{ #FF0000}{v_{\pi_1}}\\
\end{aligned}</script><p>可以发现，value iteration就是在得到第一个<script type="math/tex">v</script> 后就进行下一步操作；policy iteration则是不断你迭代直到收敛。 那么 truncated policy iteraion则是二者的结合，选择在中间的某一步停下。</p>
<h2 id="第五章-MonteCarlo-learning"><a href="#第五章-MonteCarlo-learning" class="headerlink" title="第五章 MonteCarlo learning"></a>第五章 MonteCarlo learning</h2><p>蒙特卡洛方法是一个model-free RL的方法。（前面讲的算法都是model-based RL方法）</p>
<h3 id="抛硬币例子"><a href="#抛硬币例子" class="headerlink" title="抛硬币例子"></a>抛硬币例子</h3><p>假设抛硬币问题： 抛一个硬币，正面价值为1，反面为-1，期望是多少？</p>
<p>那么 model-based方法：直接计算数学期望undefinedmathbb{E}[X] = \underset{x}{\sum}xp(x) = 1*0.5+(-1)*0.5 = 0$</p>
<p>结果很精确，但是通常很难找到这样的数学模型。<br>model-free方法：做实验，随机扔硬币，然后统计值，最终可以得到近似值。</p>
<h3 id="一个简单的MC-based-RL算法"><a href="#一个简单的MC-based-RL算法" class="headerlink" title="一个简单的MC-based RL算法"></a>一个简单的MC-based RL算法</h3><p>（我们称这个算法为MC-basic算法）</p>
<p>可以通过改变Policy iteration算法来变成model-free 算法。</p>
<ol>
<li>policy evalution(PE): <script type="math/tex">v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}</script></li>
<li>policy improvement(PI): undefinedpi_{k+1}=\underset{\pi}{argmax}(r_\pi + \gamma P_\pi v_{\pi_k})$ </li>
</ol>
<p>undefinedpi_{k+1}(s) = \underset{\pi}{argmax} \underset{a}{\sum}\pi(a|s)\textcolor{ #FF0000}{q_{\pi_k}(s,a)}$</p>
<p>关键在于计算<script type="math/tex">q_{\pi_k}(s,a)</script> , 两种方法：</p>
<ol>
<li>需要模型：<script type="math/tex">q_{\pi_k}(s,a) = \underset{r}{\sum}p(r|s,a)r + \gamma \underset{s'}{\sum}p(s'|s,a)v_{\pi_k}(s')</script></li>
<li>不需要模型：<script type="math/tex">q_{\pi_k}(s,a) = \mathbb{E}[G_t|S_t=s,A_t=a]</script></li>
</ol>
<p>基于蒙特卡洛的model即通过大量采样来估计<script type="math/tex">G_t</script>  </p>
<h3 id="MC-exploring-Starts"><a href="#MC-exploring-Starts" class="headerlink" title="MC exploring Starts"></a>MC exploring Starts</h3><p>遵循策略undefinedpi$ ，我们会得到一个episode如下：</p>
<p><script type="math/tex">s_1 \overset{a_2}\to s_2\overset{a_4}\to s_1\overset{a_2}\to s_2\overset{a_3}\to s_5\overset{a_1}{\to}...</script></p>
<p>定义Visit；一个episode中访问的undefinedstate,action)$对的数量。</p>
<p>在MC-basic方法中，使用的是Initial-visit method，即只考虑<script type="math/tex">s_1 \overset{a_2}{\to}</script> 这一个(state,action)对。这导致了没有充分利用了整个episode。</p>
<p>那么对于一个episode:</p>
<script type="math/tex; mode=display">
\begin{aligned}
s_1 \overset{a_2}\to & s_2\overset{a_4}\to & s_1\overset{a_2}\to & s_2\overset{a_3}\to & s_5\overset{a_1}{\to}\dots & [original\ episode]
\\
&s_2\overset{a_4}\to & s_1\overset{a_2}\to &s_2\overset{a_3}\to &s_5\overset{a_1}{\to}\dots & [episode\ starting\ from (s_2,a_4)]
\\
&&s_1\overset{a_2}\to &s_2\overset{a_3}\to &s_5\overset{a_1}{\to} \dots & [episode\ starting\ from (s_1,a_2)]
\\
&&& s_2\overset{a_3}\to &s_5\overset{a_1}{\to} \dots & [episode\ starting\ from (s_2,a_3)]
\\
&&&&s_5\overset{a_1}{\to} \dots & [episode\ starting\ from (s_5,a_1)]
\end{aligned}</script><p>因此我们就可以通过这一个episode来估计undefineds_1,a_2),(s_2,a_4),(s_1,a_2),(s_2,a_3),(s_5,a_1),\dots<script type="math/tex">的action value。而不是仅仅用于</script>(s_1,a_2)$ 。</p>
<ul>
<li>first-visist : 指在遇到相同的(state,action)时，只使用第一次遇到的。</li>
<li>every-visit：指在遇到相同的(state,action)时，每个都做考虑，最后综合起来。</li>
</ul>
<p>generalized policy iteration(广义策略迭代)：指并不是精确求解的代码，使用迭代来得到策略，像truncated policy iteration algorithm和 MC都属于generalized policy iteration 。</p>
<h3 id="soft-policies"><a href="#soft-policies" class="headerlink" title="soft policies"></a>soft policies</h3><p>因为我们从一个(state,action)出发能够到达多个状态，所以我们也就没必要把所有的(state,action)都设置为出发点了。</p>
<p>那么如何选择出发点？</p>
<p>undefinedepsilon-greedy\ policies<script type="math/tex">:</script>\pi(a|s)= \begin{equation}   \left\{               \begin{array}{**lr**}   1-\frac{\epsilon}{|A(s)|}(|A(s)|-1) &amp;for\ the\ greedy\ action  \\ \frac{\epsilon}{|A(s)|}  &amp; for\ the\ other\ |A(s)|-1\ actions \end{array}   \right.   \end{equation}$</p>
<p>这里的greedy action指的就是<script type="math/tex">q_\pi(s,a^*)</script> 最大的那个action。(undefinedepsilon$通常很小)， 这样在保证greedy action被选择的概率较大的情况下，其他的action同样有一些概率被选择。</p>
<ul>
<li>undefinedepsilon-greedy\ policies<script type="math/tex">能够平衡</script>exploitation<script type="math/tex">和</script>exploration$ </li>
</ul>
<p>exploitation：指的是充分利用value，贪心于当前。</p>
<p>exploration：指的是探索当前非最佳的情况，可能会找到未来更优的情况。</p>
<p>这样选择一个(state,action)作为出发点，就可以通过exploration来得到所有的(state,action)的策略。</p>
<h3 id="MC-undefinedepsilonundefinedGreedy-algorithm"><a href="#MC-undefinedepsilonundefinedGreedy-algorithm" class="headerlink" title="MC undefinedepsilonundefinedGreedy algorithm"></a>MC undefinedepsilonundefinedGreedy algorithm</h3><p>对于之前的方法，只会选择最优的action，即<script type="math/tex">a^*</script> 。</p>
<p>undefinedpi_{k+1}(s) = arg\ \underset{\textcolor{ #0000FF}{\pi \in \Pi_{\epsilon}}}{max}\underset{a}{\sum}\pi(a|s)q_{\pi_k}(s,a)$</p>
<p>那么对于MC undefinedepsilonundefinedGreedy algorithm </p>
<script type="math/tex; mode=display">
\pi(a|s)= \begin{equation}   \left\{               \begin{array}{**lr**}   1-\frac{\epsilon}{|A(s)|}(|A(s)|-1) & a=a_k^*\\ \frac{\epsilon}{|A(s)|}  & a \not=a_k^* \end{array}   \right.   \end{equation}</script><p>便是给了其他action一个较小的undefinedfrac{\epsilon}{|A(s)|}$</p>
<p>undefinedepsilonundefinedGreedy algorithm 中的undefinedepsilon$ 较大的时候，探索性很强，但是最优性比较差。</p>
<p>我们可以通过起初设置较大的undefinedepsilon$，然后逐渐减小他来平衡探索性和最优性。</p>
<h2 id="第六章-Stochastic-Approximation-amp-Stochastic-Grandient-Descent"><a href="#第六章-Stochastic-Approximation-amp-Stochastic-Grandient-Descent" class="headerlink" title="第六章 Stochastic Approximation &amp; Stochastic Grandient Descent"></a>第六章 Stochastic Approximation &amp; Stochastic Grandient Descent</h2><p>Stochastic Approximation（随机近似理论） 和Stochastic Grandient Descent（随机梯度下降）</p>
<h3 id="Motivating-example"><a href="#Motivating-example" class="headerlink" title="Motivating example"></a>Motivating example</h3><p>mean estimation problem：</p>
<ol>
<li>考虑有一个随机变量 X</li>
<li>目标是计算期望 undefinedmathbb{E}[X]$</li>
<li>假设我们有N个采样undefined{x_i\}_{i=1}^N$</li>
<li>那么期望可以被估计为undefinedmathbb{E}[X] \approx \overline{x}:=\frac{1}{N}\underset{i=1}{\overset{N}{\sum}}x_i$</li>
<li>当<script type="math/tex">N \to \infty</script> ，undefinedoverline{x} \to \mathbb{E}[X]$</li>
</ol>
<p>怎么计算<script type="math/tex">mean\ \overline{x}</script> ?</p>
<p>方法一： 计算所有的总和，然后除以<script type="math/tex">N</script></p>
<p>方法二（iterative mean estimation)：实时估计undefinedoverline{x}<script type="math/tex">，当出现新的</script>x_i<script type="math/tex">时，更新</script>\overline{x}$</p>
<p>我们规定<script type="math/tex">w_{k+1}</script> 表示前undefinedtextcolor{ #0000EE}{k}<script type="math/tex">个x的均值。即</script>w_{k+1} = \frac{1}{k}\underset{i=1}{\overset{k}{\sum}}x_i <script type="math/tex">。（一般设置</script>w_1 = x_1undefined</p>
<p>那么根据如下公式</p>
<script type="math/tex; mode=display">
\begin{aligned}
w_{k+1} = \frac{1}{k}\underset{i=1}{\overset{k}{\sum}}x_i & = \frac{1}{k}(\frac{1}{k-1}\underset{i=1}{\overset{k-1}{\sum}}x_i+x_k) \\ 
&=\frac{1}{k}((k-1)w_k + x_k) = w_k - \frac{1}{k}(w_k-x_k)
\end{aligned}</script><p>我们就可以迭代地计算<script type="math/tex">w_{k+1}</script> 了。</p>
<p>于是我们稍作改进，把undefinedfrac{1}{k}<script type="math/tex">换成</script>\alpha<script type="math/tex">.于是我们就可以通过调整</script>\alpha$ 来改变公式的计算了。</p>
<script type="math/tex; mode=display">
w_{k+1} = w_k - \alpha (w_k-x_k)</script><h3 id="Robbins-Monro-algorithm"><a href="#Robbins-Monro-algorithm" class="headerlink" title="Robbins-Monro algorithm"></a>Robbins-Monro algorithm</h3><blockquote>
<p> stochastic approximation能够做到在不知道函数具体公式的情况下求出解。</p>
<p>RM算法是stochastic approximation中的开创性工作。</p>
<p>而stochastic gradient descent algorithm 则是RM的一种特殊情况。</p>
</blockquote>
<p>问题：求解<script type="math/tex">g(w) = 0</script>方程， 其中<script type="math/tex">w</script>是未知量，<script type="math/tex">g</script>是函数。</p>
<p> 于是RM算法可以求解如下问题：</p>
<p><script type="math/tex">w_{k+1} = w_k - a_k \tilde g(w_k,\eta _k)</script> </p>
<p>其中undefinedeta _k<script type="math/tex">是噪声，</script>\tilde g(w_k,\eta _k) = g(w_k) + \eta_k<script type="math/tex">，</script>\alpha$是一个正数 。</p>
<blockquote>
<p>函数<script type="math/tex">g(w_k)</script> 是一个黑盒函数，我们无法得出它的具体公式。</p>
</blockquote>
<p>不断迭代这个公式，就能够收敛到<script type="math/tex">g(w) = 0</script></p>
<h3 id="RM算法-Convergence-properties"><a href="#RM算法-Convergence-properties" class="headerlink" title="RM算法-Convergence properties"></a>RM算法-Convergence properties</h3><p>RM算法的三个条件：</p>
<ol>
<li><p><script type="math/tex">0 < c_1 \le \nabla _wg(w) \le c_2</script> ，即导数大于0，并且不会趋于无穷。</p>
</li>
<li><p>undefinedsum_{k=1}^{\infty}a_k = \infty<script type="math/tex">并且</script>\sum_{k=1}^{\infty}a_k^2 &lt; \infty$ 。</p>
<p> undefinedsum_{k=1}^{\infty}a_k^2 &lt; \infty<script type="math/tex">保证了</script>a_k$一定会收敛到0。</p>
<p> undefinedsum_{k=1}^{\infty}a_k = \infty<script type="math/tex">保证了</script>a_k$收敛的不会太快，否则加起来就不会是无穷了。</p>
</li>
<li><p>undefinedmathbb{E}[\eta_k] = 0<script type="math/tex">并且</script>\mathbb{E}[n_k^2|\mathcal{H}]&lt;\infty<script type="math/tex">(这里的</script>\mathbb{E}[\eta_k^2|\mathcal{H}]<script type="math/tex">的意思是</script>\eta_k<script type="math/tex">的方差)。通常这里的噪声通过同分布(Independent and Identically Distriuted)采样得来，并且在此处</script>\eta_k$并没有强制要求满足高斯分布。</p>
</li>
</ol>
<blockquote>
<p>对于条件二的解释：</p>
<p>根据上面的公式<script type="math/tex">w_{k+1}-w_k = a_k \tilde g(w_k,\eta_k)</script> ,那么<script type="math/tex">a_k</script>收敛到0，才能保证<script type="math/tex">w_{k+1}-w_k</script>不断收敛到0，从而趋于稳定。</p>
<p>而将<script type="math/tex">k = 1,2,...,\infty</script>  的公式相加可以得到</p>
<p><script type="math/tex">w_{\infty} - w_1 = \overset{\infty}{\underset{k=1}{\sum}}a_k \tilde g(w_k,\eta_k)</script> .</p>
<p><script type="math/tex">w^* \approx w_\infty</script>是我们猜测的值，<script type="math/tex">w_1</script> 是初始值，那么undefinedsum_{k=1}^{\infty}a_k = \infty<script type="math/tex">保证了不管我们选的初始值</script>w_1<script type="math/tex">离目标值有多远，最终都可以通过不断迭代得到</script>w^*$</p>
</blockquote>
<p><script type="math/tex">a_k</script>取什么值是符合条件的？ <script type="math/tex">a_k= \frac{1}{k}</script> 。（但一般在<script type="math/tex">k</script>很大的时候，不会让<script type="math/tex">a_k</script>一直变小，达到某个较小值后则会不再改变）</p>
<h3 id="SGD-stochatic-gradient-descent"><a href="#SGD-stochatic-gradient-descent" class="headerlink" title="SGD(stochatic gradient descent)"></a>SGD(stochatic gradient descent)</h3><p>目标是解决如下优化问题：</p>
<p>undefinedunderset{w}{min} J(w) =\mathbb E[f(w,X)]$ </p>
<p>算法一、 gradient descent(GD)梯度下降法：</p>
<p><script type="math/tex">w_{k+1} = w_k- \alpha_k \nabla_w \mathbb E[f(w_k,X)] = \alpha_k\mathbb E[\nabla_wf(w_k,X)]</script> </p>
<p>这里的undefinedalpha_k$  是步长，就是学习率。 但是一般无法得到准确的梯度的期望。</p>
<p>算法二、batch gradient descent(BGD)批量梯度下降法：</p>
<p>undefinedmathbb [\nabla_w f(w_k,X)] \approx \frac{1}{n} \underset{i=1}{\overset{n}{\sum}}\nabla_wf(w_k,x_i)$ </p>
<p>于是得到：<script type="math/tex">w_{k+1} = w_k- \alpha_k\frac{1}{n} \underset{i=1}{\overset{n}{\sum}}\nabla_wf(w_k,x_i)</script></p>
<p>不需要求期望，用多次采样的平均值来代替期望值，但是每次都需要求n个数的平均太耗时了。（n为采样次数）</p>
<p>算法三、stochastic gradient descent(SGD) 随机梯度下降：</p>
<p><script type="math/tex">w_{k+1} = w_k - \alpha_k \nabla_w f(w_k,x_k)</script> </p>
<p>和GD相比，替使用随机梯度来替换准确的期望的梯度。</p>
<p>和BGD相比，其实就是把n设置为了1。</p>
<h3 id="SGD-的例子和练习"><a href="#SGD-的例子和练习" class="headerlink" title="SGD 的例子和练习"></a>SGD 的例子和练习</h3><p>假设如下例子：undefinedunderset{w}{min} \ J(w) = \mathbb E[f(w,X)] = \mathbb E[\frac{1}{2} ||w-X||^2]$ </p>
<p>此处的<script type="math/tex">f(w,X)= ||w-X||^2/2, \nabla_wf(w,X) = w - X</script></p>
<p>三个练习：</p>
<ol>
<li>证明最优解<script type="math/tex">w^*</script> 满足<script type="math/tex">w^* = \mathbb E [X]</script> </li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
& \nabla _w\ J(w) = 0 \\ 
\Rightarrow & \mathbb E[\nabla _w f(w,X)] = 0 \\
\Rightarrow & \mathbb E[w-X] = 0 \\
\Rightarrow & w^* = \mathbb E[X]
\end{aligned}</script><ol>
<li>这个例子的GD算法是什么？</li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
w_{k+1} & = w_k - \alpha_k \nabla _w J(w_k) \\
&=w_k - \alpha_k \mathbb E[\nabla _w J(w_k)] \\ 
&=w_k - \alpha_k \mathbb E[w_k - X]
\end{aligned}</script><ol>
<li>这个例子的SGD算法是什么？</li>
</ol>
<p>不求期望了，直接用某一个的<script type="math/tex">w_k - x_k</script> 来代替undefinedmathbb E[w_k - X]$</p>
<script type="math/tex; mode=display">
w_{k+1} = w_k - \alpha_k \nabla_wf(w_k,x_k) = w_k - \alpha _k (w_k - x_k)</script><p>我们发现最后的公式和mean alogrithm算法是一样的，所以mean algorithm算法就是一种特殊的SGD算法。</p>
<h3 id="SGD算法的收敛性-convergence"><a href="#SGD算法的收敛性-convergence" class="headerlink" title="SGD算法的收敛性(convergence)"></a>SGD算法的收敛性(convergence)</h3><ol>
<li>首先证明SGD是一种特殊的RM算法：</li>
</ol>
<p>SGD的目标是最小化<script type="math/tex">J(w) = \mathbb E[f(w,X)]</script> , 这个问题可以转换为寻根问题：</p>
<p>undefinednabla_w \ J(w) = \mathbb E[\nabla _w f(w,X)] = 0$</p>
<p>设<script type="math/tex">g(w) = \nabla_w J(w) = \mathbb E[\nabla_wf(w,X)]</script></p>
<p>那么SGD的目标就是找到<script type="math/tex">g(w)= 0</script>的根。</p>
<p>我们可以测量的是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde g(w,\eta) &= \nabla_w f(w,x) \\ 
  &=  \mathbb E[\nabla_wf(w,X)] + (\nabla_wf(w,x) - \mathbb E[\nabla_w f(w,X)]) 
 \end{aligned}</script><p>而与之对应的RM算法是 </p>
<script type="math/tex; mode=display">
w_{k+1}= w_k - \alpha _k \tilde g(w,\eta) = w_k - \alpha_k \nabla_wf(w_k,x_k)</script><ol>
<li>接下来我们就可以应用RM算法的收敛性条件，来证明SGD是收敛的。</li>
</ol>
<h3 id="SGD算法的收敛模式"><a href="#SGD算法的收敛模式" class="headerlink" title="SGD算法的收敛模式"></a>SGD算法的收敛模式</h3><p>SGD收敛的过程中，是否会收敛很慢或者收敛随机？</p>
<p>我们定义相对误差undefineddelta _k$</p>
<script type="math/tex; mode=display">
\delta _k = \frac{|\nabla_wf(w,x) - \mathbb E[\nabla_w f(w,X)]|}{|\mathbb E[\nabla_w f(w,X)]|}</script><p> 此处的undefinedmathbb E[\nabla_w f(w,X)]<script type="math/tex">是true gradient， 而</script>\nabla_wf(w,x)$ 是 stochastic gradient。</p>
<blockquote>
<p> 性质：当<script type="math/tex">w_k</script> 离<script type="math/tex">w^*</script> 较远时，相对误差较小， 当<script type="math/tex">w_k</script> 离<script type="math/tex">w^*</script> 很近的时候，才会有比较大的相对误差（即随机性）</p>
</blockquote>
<p>如何得到如上性质？</p>
<p>使用拉格朗日中值定理<script type="math/tex">f(x_1)-f(x_2) =f'(x_3)(x_1 - x_2)</script></p>
<p>那么由undefinedmathbb E[\nabla _wf(w^*,X)] = 0$和中值定理 ,我们有</p>
<script type="math/tex; mode=display">
\delta_k = \frac{|\nabla_wf(w,x) - \mathbb E[\nabla_w f(w,X)]|}{|\mathbb E[\nabla_w f(w,X)] - \mathbb E[\nabla_wf(w^*,X)]|} = \frac{|\nabla_wf(w,x) - \mathbb E[\nabla_w f(w,X)]|}{|\mathbb E[\nabla^2_w f(\tilde w,X)(w_k-w^*)]|}</script><p>我们假设undefinednabla^2_wf \ge c &gt; 0$ </p>
<p>那么我们考虑分母项，就有</p>
<script type="math/tex; mode=display">
\begin{aligned}
|\mathbb E\textcolor{ #FF0000}{[}\nabla^2_w f(\tilde w,X)(w_k-w^*)\textcolor{ #FF0000}{]}| &= |\mathbb E\textcolor{ #FF0000}{[}\nabla^2_w f(\tilde w,X) \textcolor{ #FF0000}{]}(w_k-w^*)| \\ 
&=|\mathbb E[\nabla^2_w f(\tilde w,X)]||(w_k-w^*)| \ge c|w_k - w^*|
\end{aligned}</script><p>于是误差undefineddelta_k$满足</p>
<script type="math/tex; mode=display">
\delta_k \le \frac{|\nabla_wf(w,x) - \mathbb E[\nabla_w f(w,X)]|}{c|w_k - w^*|}</script><p>于是当<script type="math/tex">w_k</script> 距离<script type="math/tex">w^*</script>比较远，那么分母比较大，相对误差undefineddelta_k$ 的上界比较小。</p>
<p>当<script type="math/tex">w_k</script> 距离<script type="math/tex">w^*</script>比较近，那么分母比较小，此时相对误差undefineddelta_k$ 的上界才会变大一些。</p>
<h3 id="BGD-MBGD和SGD"><a href="#BGD-MBGD和SGD" class="headerlink" title="BGD,MBGD和SGD"></a>BGD,MBGD和SGD</h3><ul>
<li>BGD(batch gradient descent) , 用到所有的采样来平均求期望</li>
<li>MBGD(min-batch gradient descent) ,选择一部分采样(m个采样)</li>
<li>SGD(stocastic gradient descent) ，选择一个采样</li>
</ul>
<p>在MBGD中，当MBGD中的采样数量<script type="math/tex">m=1</script>时，等价于SGD。</p>
<p>当采样数量<script type="math/tex">m = n</script>时，趋近于BGD(注意！此时不完全等于BGD，因为BGD是取出所有的n个样本，而MBGD是对样本集进行n次的采样)</p>
<p>考虑如下优化问题：</p>
<p>undefinedunderset{w}{min} \ J(w) = \frac{1}{2n} \underset{i=1}{\overset{n}{\sum}} ||w - x_i || ^2$ </p>
<p>那么三种算法的迭代公式如下：</p>
<script type="math/tex; mode=display">
w_{k+1} = w_k - \alpha_k \frac{1}{n} \underset{i=1}{\overset{n}{\sum}}(w_k - x_i) = w_k - \alpha_k (w_k - \overline x) & (BGD)\\
w_{k+1} = w_k - \alpha_k \frac{1}{m} \underset{j \in I_k}{\overset{n}{\sum}}(w_k - x_j) = w_k - \alpha_k (w_k - \overline x^{(m)}) &(MBGD) \\
w_{k+1} =  w_k - \alpha_k (w_k - x_k) &(SGD)</script><h2 id="第七章-Temporal-Difference-learning"><a href="#第七章-Temporal-Difference-learning" class="headerlink" title="第七章 Temporal-Difference learning"></a>第七章 Temporal-Difference learning</h2><p>Temporal-Difference learning （TD）时序差分算法</p>
<p>这是一个incremental 迭代式的算法。</p>
<h3 id="motivating-example"><a href="#motivating-example" class="headerlink" title="motivating example"></a>motivating example</h3><ol>
<li>先考虑一个简单的问题 mean estimation ： 计算</li>
</ol>
<p><script type="math/tex">w = \mathbb[X]</script> , (X是一些iid(独立同分布)采样undefined{x\}undefined</p>
<p>令<script type="math/tex">g(w) = w - \mathbb E[X]</script> ,则有</p>
<p>undefinedtilde g(w,\eta) = w - x = (w - \mathbb E[X]) + (\mathbb E[X] - x) \approx g(w) + \eta$ </p>
<p>然后根据RM算法，可以得到<script type="math/tex">w_{k+1} = w_k - \alpha_k \tilde g(w_k,\eta_k) = w_k - \alpha _k (w_k - x_k)</script></p>
<ol>
<li>考虑一个复杂一些的例子：计算</li>
</ol>
<p><script type="math/tex">w = \mathbb E[v(X)]</script> , (X是一些iid(独立同分布)采样undefined{x\}undefined</p>
<p>令<script type="math/tex">g(w) = w - \mathbb E[v(X)]</script></p>
<p>undefinedtilde g(w,\eta) = w - v(x) = (w - \mathbb E[X]) + (\mathbb E[X] - v(x)) \approx g(w) + \eta$</p>
<p>然后根据RM算法，可以得到<script type="math/tex">w_{k+1} = w_k - \alpha_k \tilde g(w_k,\eta_k) = w_k - \alpha _k (w_k - v(x_k))</script></p>
<ol>
<li>第三个例子：计算</li>
</ol>
<p><script type="math/tex">w = \mathbb [R + \gamma v(X)]</script> , (<script type="math/tex">R,X</script> 是随机变量，undefinedgamma<script type="math/tex">是常量，</script>v(\cdot)$ 是函数)</p>
<p>令<script type="math/tex">g(w) = w - \mathbb E[R + \gamma v(X)]</script> ,</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde g(w,\eta) &= w - \mathbb E[R+\gamma v(X)] \\
& = (w - \mathbb E[R + \gamma v(X)]) + (\mathbb E[R + \gamma v(X)] - [r + \gamma v(X)]) \\
& \approx g(w) + \eta
\end{aligned}</script><p>然后根据RM算法，可以得到undefinedtextcolor{ #0000FF}{w_{k+1} = w_k - \alpha_k \tilde g (w_k,\eta_k) = w_k - \alpha_k[w_k -(r_k + \gamma v(x_k))]}$</p>
<h3 id="TD算法中的state-values"><a href="#TD算法中的state-values" class="headerlink" title="TD算法中的state values"></a>TD算法中的state values</h3><blockquote>
<p>注意：</p>
<ul>
<li>TD算法通常指的是一大类的RL算法。</li>
<li>TD算法也可以特指一种用于估计state values的算法。</li>
</ul>
</blockquote>
<p> TD算法基于数据：undefineds_0,r_1,s_1,…,s_t,r_{t+1},s_{t+1},…)<script type="math/tex">或者</script>\{(s_t,r_{t+1},s_{t+1})\}_t<script type="math/tex">，这种数据通过给定的策略</script>\pi$ 来生成。</p>
<p>TD算法则是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{t+1}(s_t) &= v_t(s_t) - \alpha_t(s_t)[v_t(s_t) - [r_{t+1} + \gamma v_t(s_{t+1})]] & (1) \\
v_{t+1}(s) &=v_t(s), \forall s \not=s_t&(2)
\end{aligned}</script><p>对于公式undefined2)<script type="math/tex">表示，如果现在的状态是</script>s_t$ ，那么其他状态的value是不更新的。 </p>
<p>我们关注于第一个式子：</p>
<p><script type="math/tex">v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t)[v_t(s_t) - [r_{t+1} + \gamma v_t(s_{t+1})]]</script></p>
<p>其中的<script type="math/tex">v_{t+1}(s_t)</script> 是新的估计值，<script type="math/tex">v_t(s_t)</script> 是现在的估计值。</p>
<p><script type="math/tex">v_t(s_t) - [r_{t+1} + \gamma v_t(s_{t+1})]</script>  是误差undefineddelta_t$</p>
<p><script type="math/tex">[r_{t+1} + \gamma v_t(s_{t+1})]</script> 是目标undefinedoverline v_t$</p>
<blockquote>
<p> 为什么undefinedoverline v_t<script type="math/tex">是“TD目标” ？因为每次</script>v(s_t)<script type="math/tex">都会向着</script>\overline v_t$ 移动。</p>
<script type="math/tex; mode=display">
\begin{aligned}
& v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t)[v_t(s_t) - \overline v_t] \\
\Rightarrow &  v_{t+1}(s_t) \textcolor{ #0000FF}{-\overline v_t}= v_t(s_t) \textcolor{ #0000FF}{-\overline v_t} - \alpha_t(s_t)[v_t(s_t) - \overline v_t ] \\
\Rightarrow &  v_{t+1}(s_t) \textcolor{ #0000FF}{-\overline v_t}= [1 -\alpha_t(s_t)] [v_t(s_t) \textcolor{ #0000FF}{-\overline v_t} ] 
\end{aligned}</script><p>因为<script type="math/tex">0 < 1 - \alpha_t(s_t) < 1</script> </p>
<p>于是<script type="math/tex">|v_{t+1}(s_t) - \overline{v}_t| \le |v_{t}(s_t) - \overline{v}_t|</script></p>
<p>为什么 undefineddelta_t$是“TD error”？</p>
<p>undefineddelta_t = v(s_t) - [r_{t+1} + \gamma v(s_{t+1})]$</p>
<p>因为发生在t和t+1两个时刻 ，所以才叫时序差分，</p>
<p>TD error 描述了<script type="math/tex">v_t</script> 和<script type="math/tex">v_\pi</script> 之间的误差。</p>
<p>当<script type="math/tex">v_t = v_\pi</script> 时，那么应该有undefineddelta _t = 0$ 。</p>
<p>TD error是一种 innovation，这是经验undefineds_t,r_{t+1},s_{t+1})$的一种新的信息。</p>
</blockquote>
<h3 id="TD算法的数学意义"><a href="#TD算法的数学意义" class="headerlink" title="TD算法的数学意义"></a>TD算法的数学意义</h3><p>他解决了给定undefinedpi$，求解贝尔曼公式。</p>
<p>新的贝尔曼公式：</p>
<script type="math/tex; mode=display">
v_\pi(s) = \mathbb E[R + \gamma G |S = s], s \in S</script><p>在这之中G是下个状态的Reward，所以undefinedmathbb E[G|S = s] $可以表示为：</p>
<script type="math/tex; mode=display">
\mathbb E[G|S = s] = \underset{a}{\sum} \pi(a|s) \underset{s'}{\sum} p(s'|s,a) v_\pi(s') = \mathbb E[v_\pi(S')|S = s]</script><p>其中<script type="math/tex">S'</script> 是下一个状态</p>
<p>于是s的state value可以写为：</p>
<script type="math/tex; mode=display">
v_\pi(s) = \mathbb E[R + \gamma v_\pi(S')| S = s],s \in S</script><p>这个公式也被称为贝尔曼期望公式。</p>
<p>接下来使用RM算法来求解这个贝尔曼期望公式：<br>定义<script type="math/tex">g(v(s)) = v(s) - \mathbb E[R + \gamma v_\pi(S')| S = s] = 0</script></p>
<p>于是我们有<script type="math/tex">g(v(s)) = 0</script> </p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde g(v(s)) &= v(s) - [r + \gamma v_\pi(s')] \\
 &= (v(s) - \mathbb E[R + \gamma v_\pi(S')| s]) + (\mathbb E[R + \gamma v_pi(S')| s] - [r + \gamma v_\pi (s')]) 
\end{aligned}</script><p>在这之中，<script type="math/tex">g(v(s)) = (v(s) - \mathbb E[R + \gamma v_\pi(S')| s])</script> ,误差undefinedeta = E[R + \gamma v_pi(S’)| s] - [r + \gamma v_\pi (s’)]) $</p>
<p>那么与之对应的RM算法是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{k+1} (s) &= v_k(s) - \alpha_k \tilde g(v_k(s)) \\ 
&= v_k(s) - \alpha_k (v_k(s)-[r_k+\gamma v_\pi(s'_k)]) , k =1,2,3,...
\end{aligned}</script><p>这里的<script type="math/tex">v_k(s)</script>代表<script type="math/tex">v_\pi(s)</script>在第k步的估计，而<script type="math/tex">r_k,s'_k</script> 是第k步中从<script type="math/tex">R,S'</script>中取出的样本。</p>
<p>对公式做以下替换：</p>
<ul>
<li><p>将一组采样undefined{(s,r,s’)\}<script type="math/tex">替换为一组序列</script>\{s_t,r_{t+1},s_{t+1}\}$， 从而做到对所有的s都进行更新。</p>
</li>
<li><p>将<script type="math/tex">v_\pi(s')</script> 换为<script type="math/tex">v_k(s'_k)</script> ，即我们直接用<script type="math/tex">s'</script> 在第k步的估计值来替代真实值。 虽然会有一些偏差，但是最终会收敛到<script type="math/tex">v_\pi</script> </p>
</li>
</ul>
<blockquote>
<p>TD算法的收敛：</p>
<p>对于所有状态<script type="math/tex">s \in S</script> 。当<script type="math/tex">t \to \infty</script> 时，  <script type="math/tex">v_t(s)</script> 以概率1收敛到策略undefinedpi<script type="math/tex">下的状态值函数</script>v_\pi(s)$。</p>
<p>如果对于所有的状态<script type="math/tex">s \in S</script> ，步长参数序列undefinedalpha_t(s)<script type="math/tex">都满足</script>\sum_t\alpha_t = \infty<script type="math/tex">并且</script>\sum_t\alpha_t^2(s) &lt; \infty$ 那么上述收敛成立。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>TD/Sarsa learning</th>
<th>MC learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>online：TD学习是在线的，在接收到一个奖励后可以更新state/action value</td>
<td>Not online：MClearning是非在线的，必须等到整个episode已经完成之后，计算return值然后进行估计。</td>
</tr>
<tr>
<td>continuing tasks：即能处理一直持续下去的任务，同时也能解决episodic tasks。</td>
<td>Episodic tasks：必须是有限步的episode，才能等到他的返回值。</td>
</tr>
<tr>
<td>Bootstrapping：会基于之前对状态的猜测，加上一些新的信息来形成一个新的猜测</td>
<td>Non-boostrapping：直接根据当前的episode计算return，不涉及到之前的估计值</td>
</tr>
<tr>
<td>Low estimation variance ：在算法过程中涉及到的随机变量比较少，所以方差会比较小</td>
<td>High estimation variance：它涉及到了很多的variable，因为一次episode会涉及到很多的Reward，而只用其中一次的采样，所以就会有比较大的方差。</td>
</tr>
<tr>
<td>bias：因为基于之前的经验，所以可能会因为之前的经验而产生bias，导致有偏估计，但是在不断增加经验后还是会趋于正确结果</td>
<td>no bias：不基于之前的估计，所以不会产生bias</td>
</tr>
</tbody>
</table>
</div>
<h3 id="TD算法中的action-values：Sarsa"><a href="#TD算法中的action-values：Sarsa" class="headerlink" title="TD算法中的action values：Sarsa"></a>TD算法中的action values：Sarsa</h3><blockquote>
<p> Sarsa是经验集undefineds_t,a_t,r_{t+1},s_{t+1},a_{t+1})$ 的拼接。</p>
</blockquote>
<p>TD算法是用来估计给定策略undefinedpi$ 的state value，但我们需要估计的是action value。下面引入Sarsa。</p>
<p>假设我们有如下经验undefined{(s_t,a_t,r_{t+1},s_{t+1},a_{t+1}) \}_t$  ，那么我们定义Sarsa公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
q_{t+1}(s_t,a_t) &= q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t) - [r_{t+1} + \gamma q_t(s_{t+1},a_{t+1})]] \\
 q_{t+1}(s,a) &=q_t(s,a), \forall(s,a) \not= (s_t,a_t) 
 \end{aligned}</script><p>这个式子和TD算法几乎一样，只是类似地把<script type="math/tex">v_t(s_t)</script> 改成了<script type="math/tex">q_t(s_t,a_t)</script>这样子。</p>
<p>Sarsa的数学意义和TD也是几乎一样的。（如贝尔曼公式，收敛性等）</p>
<p>Sarsa所求解的贝尔曼公式：</p>
<script type="math/tex; mode=display">
q_\pi(s,a) = \mathbb E [R+ \gamma q_\pi(S',A')|s,a], \forall s,a</script><ol>
<li><p>收集经验：undefineds_t,a_t,r_{t+1},s_{t+1},a_{t+1})<script type="math/tex">,遵循</script>\pi_t(s_t)<script type="math/tex">执行</script>a_t<script type="math/tex">，得到</script>r_{t+1}<script type="math/tex">的奖励，然后走到状态</script>s_{t+1}<script type="math/tex">并遵循</script>\pi_{t}(s_{t+1})<script type="math/tex">来采取行动</script>a_{t+1}$ 。</p>
</li>
<li><p>更新q值(q value update/policy evaluaton)：<script type="math/tex">q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t) - [r_{t+1} + \gamma q_t(s_{t+1},a_{t+1})]]</script></p>
</li>
<li><p>更新策略policy(policy update/policy improvement)：</p>
<script type="math/tex; mode=display">
 \begin{aligned}
 \pi_{t+1}(a|s_t) &= 1 - \frac{\epsilon}{|\Alpha|}(|\Alpha-1|),&\ if \ a = arg\ max_aq_{t+1}(s_t,a) \\
 \pi_{t+1}(a|s_t) &=\frac{\epsilon}{|\Alpha|} ,&otherwise
 \end{aligned}</script></li>
</ol>
<p>注意这里的PE和PI是立刻执行的，而不是等return之后再精确计算。</p>
<p>注意这个策略是一个undefinedepsilon- greedy$策略，也就是说倾向于采取qvalue最大的action，但是其他的action同样有概率取到。</p>
<h3 id="Expected-Sarsa"><a href="#Expected-Sarsa" class="headerlink" title="Expected Sarsa"></a>Expected Sarsa</h3><p>公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
q_{t+1}(s_t,a_t) &= q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t)-\textcolor{ #0000FF}{(r_{i+1} + \gamma \mathbb E [q_t(s_{t+1},A)])}] 
\\
q_{t+1}(s,a) & = q_t(s,a) , \forall(s,a) \not=(s_t,a_t)
\end{aligned}</script><p>此处的undefinedmathbb E[q_t(s_{t+1},A)] = \underset{\pi}{\sum}\pi_t(a| s_{t+1})q_t(s_{t+1},a) \approx v_t(s_{t+1})$ </p>
<p>和普通的sarsa的区别是用undefinedr_{i+1} + \gamma \mathbb E [q_t(s_{t+1},A)])<script type="math/tex">替换了</script>r_{t+1}+\gamma q_t(s_{t+1},a_{t+1})undefined</p>
<p>不再需要<script type="math/tex">a_{t+1}</script> 了,随机性会减小一些，但是需要更大的计算量。</p>
<p>Expected Sarsa的数学意义也是在求解贝尔曼公式：</p>
<p><script type="math/tex">q_\pi(s,a) = \mathbb E[R_{t+1} + \gamma \mathbb E_{A_{t+1} \sim \pi(S_{t+1})}[q_\pi(S_{t+1},A_{t+1})]|S_t = s,A_t = a]</script></p>
<h3 id="n-step-Sarsa"><a href="#n-step-Sarsa" class="headerlink" title="n-step Sarsa"></a>n-step Sarsa</h3><p>是Sarsa的一个推广，包含了Sarsa和蒙德卡罗方法。</p>
<p> 我们的action value如下定义：<script type="math/tex">q_\pi(s,a) = \mathbb E[G_t|S_t=a,A_t=a]</script></p>
<p>那么<script type="math/tex">G_t</script> 可以被写成如下形式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{Sarsa} \leftarrow & G_t^{(1)} = R_{t+1} + \gamma q_\pi(S_{t+1},A_{t+1}) \\
&G_t^{(2)} = R_{t+1} + \gamma R_{t+1} + \gamma ^2 q_\pi(S_{t+2},A_{t+2}) \\
& ... \\ 
\text{n-step Sarsa}\leftarrow &G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^n q_\pi(S_{t+n},A_{t+n}) \\
& ... \\
MC \leftarrow & G_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} +  \gamma^2R_{t+3}...
\end{aligned}</script><p>所以n-step Sarsa对应的贝尔曼公式是：</p>
<script type="math/tex; mode=display">
q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t) - [r_{t+1} + \gamma r_{t+2} + ... + \gamma^n q_t(s_{t+n},a_{t+n})]]</script><p>n-step Sarsa需要的数据是undefineds_t,a_t,r_{t+1},s_{t+1},a_{t+1},…,r_{t+n},s_{t+n},a_{t+n})$</p>
<p>所以他的数据需要等到<script type="math/tex">t+n</script> 时刻，才能进行更新。是online和offline的结合。</p>
<ul>
<li>当n比较大的时候，更接近于MC，会有比较大的variance，比较小的bias。</li>
<li>当n比较小的时候，更接近于Sarsa，会有比较小的variance，比较大的bias。</li>
</ul>
<h3 id="TD中最优action-value学习-Q-learning"><a href="#TD中最优action-value学习-Q-learning" class="headerlink" title="TD中最优action value学习:Q-learning"></a>TD中最优action value学习:Q-learning</h3><p>算法如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
q_{t+1}(s_t,a_t) & = q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t) - \textcolor{ #0000FF}{[r_{t+1} + \gamma \underset{\alpha \in \mathcal A}{max}\ q_t(s_{t+1},a)]}] 
\\
q_{t+1}(s,a) & = q_t(s,a) , \forall(s,a) \not=(s_t,a_t)
\end{aligned}</script><p>和Sarsa相比，用<script type="math/tex">r_{t+1} + \gamma \underset{\alpha \in \mathcal A}{max}\ q_t(s_{t+1},a)</script> 替换了<script type="math/tex">r_{t+1} + \gamma q_t(s_{t+1},a_{t+1})</script> </p>
<p>Q-learning求解的数学问题是（不是在求解贝尔曼方程）：</p>
<p>求解一个贝尔曼最优方程：</p>
<script type="math/tex; mode=display">
q(s,a) = \mathbb E [R_{t+1}+\gamma \underset{a}{max}q(S_{t+1},a)| S_t = s,A_t = a], \forall s,a</script><h3 id="off-policy-和-on-policy"><a href="#off-policy-和-on-policy" class="headerlink" title="off-policy 和 on-policy"></a>off-policy 和 on-policy</h3><p>两种策略：</p>
<ol>
<li>behavior policy用来生成经验样本</li>
<li>target policy不断地更新来将target policy更新到optimal policy。</li>
</ol>
<p>基于这两种策略，可以分为两类算法：</p>
<ul>
<li>on-policy： 其中的behavior policy和target policy是相同的，即用自己的策略来和环境交互，然后得到经验并改进自己的策略，之后再用相同的策略和环境交互。</li>
<li>off-policy：用一个策略和环境交互得到大量经验，然后用这些经验来不断改进策略（一步到位，不再通过新的策略引入新的经验）</li>
</ul>
<p>on-policy的好处就是可以不断接收新的经验，实时更新策略。</p>
<p>off-policy的好处就是可以直接使用别人已经获取过的经验。如用之前通过探索性较强的算法得到的经验。</p>
<p>如何判断一个TD算法是on-policy还是off-policy？</p>
<ol>
<li>看这个TD算法是在解决什么样的数学问题</li>
<li>看在算法的执行过程中需要什么东西才能使算法跑起来</li>
</ol>
<ul>
<li>Sarsa是on-policy的：</li>
</ul>
<p>Sarsa在数学上就是在求解一个贝尔曼公式：</p>
<script type="math/tex; mode=display">
q_\pi(s,a) = \mathbb E [R+ \gamma q_\pi(S',A')|s,a], \forall s,a</script><p>此处的<script type="math/tex">R \sim p(R|s,a),S' \sim p(S'|s,a),\textcolor{0xFF0000}{A' \sim \pi(A'|S')}</script></p>
<p>Sarsa在算法中：</p>
<script type="math/tex; mode=display">
q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t) - [r_{t+1} + \gamma q_t(s_{t+1},a_{t+1})]]</script><blockquote>
<ol>
<li><p>如果给定了undefineds_t,a_t)<script type="math/tex">那么</script>r_{t+1}<script type="math/tex">和</script>s_{t+1}<script type="math/tex">和任何策略无关，和</script>p(r|s,a),p(s’|s,a)$有关。</p>
</li>
<li><p><script type="math/tex">a_{t+1}</script>是由策略undefinedpi_t(s_{t+1})<script type="math/tex">产生。 所以</script>\Pi_t$ 既是behavior policy也是target policy</p>
</li>
</ol>
</blockquote>
<ul>
<li>MC learning 是on-policy的：</li>
</ul>
<p>MC目的是求解如下贝尔曼方程：</p>
<script type="math/tex; mode=display">
q_\pi(s,a) = \mathbb E [R_{t+1} + \gamma R_{t+2} + ...| S_t =s,A_t = a]</script><p>MC的实现是</p>
<script type="math/tex; mode=display">
q(s,a) \approx r_{t+1} + \gamma r_{t+2} + ...</script><blockquote>
<p>我们用策略undefinedPi<script type="math/tex">来得到trajectory经验，然后得到return来近似估计</script>q_\pi<script type="math/tex">进而改进</script>\Pi$</p>
</blockquote>
<ul>
<li>Q learning 是off-policy的：</li>
</ul>
<p>Q learning求解的数学问题是：</p>
<p>求解贝尔曼最优公式：</p>
<script type="math/tex; mode=display">
q(s,a) = \mathbb E [R_{t+1}+\gamma \underset{a}{max}q(S_{t+1},a)| S_t = s,A_t = a], \forall s,a</script><p>Q learning的实现过程是：</p>
<script type="math/tex; mode=display">
q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t) - [r_{t+1} + \gamma \underset{\alpha \in \Alpha}{max}\ q_t(s_{t+1},a)]]</script><p>需要的经验是undefineds_t,a_t,r_{t+1},s_{t+1})$ </p>
<p>注意这里的经验不包含<script type="math/tex">a_{t+1}</script></p>
<blockquote>
<p>如果undefineds_t,a_t)<script type="math/tex">给定，那么</script>r_{t+1}<script type="math/tex">和</script>s_{t+1}$ 不依赖于策略。</p>
<p>behavior policy是从<script type="math/tex">s_t</script> 出发得到<script type="math/tex">a_t</script></p>
<p>target policy 是根据<script type="math/tex">q_\pi</script> 来选择action</p>
</blockquote>
<h3 id="Q-learning-的实施"><a href="#Q-learning-的实施" class="headerlink" title="Q-learning 的实施"></a>Q-learning 的实施</h3><p>如果将Q-learning中的behavior policy 和target policy强行设置为一致的，那么它可以是on-policy的：</p>
<ol>
<li><p>对每个episode执行以下三步</p>
</li>
<li><p>收集经验undefineds_t,a_t,r_{t+1},s_{t+1})<script type="math/tex">，在这一步根据</script>\pi_t(s_t)<script type="math/tex">采取行动</script>a_t<script type="math/tex">来生成</script>(r_{t+1},s_{t+1})$</p>
</li>
<li><p>更新q-value：<script type="math/tex">q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t) - [r_{t+1} + \gamma max_a \ q_t(s_{t+1},a)]]</script></p>
</li>
<li><p>更新policy：</p>
<script type="math/tex; mode=display">
 \begin{aligned}
 \pi_{t+1}(a|s_t) &= 1- \frac{\epsilon}{|\Alpha|}(|\Alpha|-1) \text{ if } a = \underset{a}{argmax}\ q_{t+1}(s_t,a) \\ 
 \pi_{t+1}(a|s_t) &= \frac{\epsilon}{|\Alpha|} \text{ otherwise}
 \end{aligned}</script></li>
</ol>
<p>也可以是off-policy的：</p>
<ol>
<li><p>对每个episode生成策略undefinedpi_b$ (这里的b代表behavior),这个策略用来生成experience</p>
</li>
<li><p>对episode的每一步<script type="math/tex">t = 0,1,2,...</script> 执行以下两步：</p>
</li>
<li><p>更新q-value:<script type="math/tex">q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t) - [r_{t+1} + \gamma max_a \ q_t(s_{t+1},a)]]</script></p>
</li>
<li><p>更新target policy:</p>
<script type="math/tex; mode=display">
 \begin{aligned}
 \pi_{T,t+1}(a|s_t) &= 1 \text{ if } a = \underset{a}{argmax}\ q_{t+1}(s_t,a) \\ 
 \pi_{T,t+1}(a|s_t) &= 0 \text{ otherwise}
 \end{aligned}</script></li>
</ol>
<p>注意这里的第三步是greedy不是undefinedepsilon-greedy<script type="math/tex">，因为我们不需要新的策略来生成经验，所以也就不需要使用</script>\epsilon-greedy$ 来增加探索性，只需要保证最优性。</p>
<p>使用off-policy的话，使用的behavior policy最好是探索度比较强的策略，否则可能得不到好的target policy。</p>
<h3 id="TD的统一表示"><a href="#TD的统一表示" class="headerlink" title="TD的统一表示"></a>TD的统一表示</h3><p>所有的TD算法都能用如下公式表达：</p>
<script type="math/tex; mode=display">
q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha _t(s_t,a_t)[q_t(s_t,a_t)-\textcolor{ #0000FF}{\overline{q}_t}]</script><p>这里的undefinedoverline{q}_t$ 就是TD target。 </p>
<p>TD算法的目标就是接近TD target ，减小TD error</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>undefinedoverline q_t$ 的表示</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sarsa</td>
<td>undefinedoverline q_t = r_{t+1} + \gamma q_t(s_{t+1}，a_{t+1})$</td>
</tr>
<tr>
<td>n-step Sarsa</td>
<td>undefinedoverline q_t = r_{t+1} + \gamma r_{t+2} +… + \gamma ^ nq_t(s_{t+n}，a_{t+n})$</td>
</tr>
<tr>
<td>Expected Sarsa</td>
<td>undefinedoverline q_t = r_{t+1} + \gamma \underset{a}{\sum}\pi_i(a</td>
<td>s_{t+1})q_t(s_{t+1},a)$</td>
</tr>
<tr>
<td>Q-learning</td>
<td>undefinedoverline q_t = r_{t+1} + \gamma \underset{a}{max}q_t(s_{t+1},a)$</td>
</tr>
<tr>
<td>Monte Carlo</td>
<td>undefinedoverline q_t = r_{t+1} + \gamma r_{t+2} + \gamma^2r_{t+3} +…$</td>
</tr>
</tbody>
</table>
</div>
<h2 id="第八章-value-function-Approximation"><a href="#第八章-value-function-Approximation" class="headerlink" title="第八章 value function Approximation"></a>第八章 value function Approximation</h2><p>在此之前，所有的state value和action value都是用表格表示出来的，例如：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><script type="math/tex">a_1</script></th>
<th><script type="math/tex">a_2</script></th>
<th><script type="math/tex">a_3</script></th>
<th><script type="math/tex">a_4</script></th>
<th><script type="math/tex">a_5</script></th>
</tr>
</thead>
<tbody>
<tr>
<td><script type="math/tex">s_1</script></td>
<td><script type="math/tex">q_\pi(s_1,a_1)</script></td>
<td><script type="math/tex">q_\pi(s_1,a_2)</script></td>
<td><script type="math/tex">q_\pi(s_1,a_3)</script></td>
<td><script type="math/tex">q_\pi(s_1,a_4)</script></td>
<td><script type="math/tex">q_\pi(s_1,a_5)</script></td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td><script type="math/tex">s_9</script></td>
<td><script type="math/tex">q_\pi(s_9,a_1)</script></td>
<td><script type="math/tex">q_\pi(s_9,a_2)</script></td>
<td><script type="math/tex">q_\pi(s_9,a_3)</script></td>
<td><script type="math/tex">q_\pi(s_9,a_4)</script></td>
<td><script type="math/tex">q_\pi(s_9,a_5)</script></td>
</tr>
</tbody>
</table>
</div>
<p>使用表格的好处就是可以直观地分析</p>
<p>坏处就是无法处理很大的state space 或者action space、无法处理连续的state和action。泛化能力不强。</p>
<h3 id="Value-Function-Approximation的含义"><a href="#Value-Function-Approximation的含义" class="headerlink" title="Value Function Approximation的含义"></a>Value Function Approximation的含义</h3><p>使用直线来拟合点：</p>
<p>undefinedhat v(s,w) = as + b = [s,1]\bigl[ \begin{smallmatrix}a \\b \end{smallmatrix}\bigr] = \phi ^T(s)w$</p>
<blockquote>
<p>这里的w是parameter vector(参数向量)</p>
<p>undefinedphi(s)$ 是s的feature vector(特征向量)</p>
<p>undefinedhat v (s,w)$ 是w的linear(线性关系)</p>
</blockquote>
<p>使用函数来拟合可以节省存储空间(只需要存w的值(a,b)即可)</p>
<p>缺点是拟合后不太精确。</p>
<p>同样可以用二次函数来拟合：</p>
<p>undefinedhat v (s,w) = as^2 + bs + c = \phi^T(s)w$</p>
<p>(需要注意的是，这样的曲线对于w来说同样是一种线性的拟合)</p>
<ul>
<li>使用Value Function Approximation的优点：</li>
</ul>
<ol>
<li>便于存储，只需要存储w即可，不用存储大量数据。</li>
<li>泛化能力更强，假设s2进行了更改，在表格存储中只会更改s2对应的内容，而使用value function approximation则会改变w的值，从而影响其他的值（如s1和s3），这样会增强泛化能力</li>
</ol>
<h3 id="objective-funciton"><a href="#objective-funciton" class="headerlink" title="objective funciton"></a>objective funciton</h3><p>令<script type="math/tex">v_\pi(s)</script> 作为真正的state value。 undefinedhat v(s,w)$是函数的近似值。</p>
<p>我们目标就是找到最优的w来使得undefinedhat v(s,w)<script type="math/tex">可以很好的拟合出</script>v_\pi(s)$</p>
<p>目标函数objective function如下：</p>
<script type="math/tex; mode=display">
J(w) = \mathbb E[(v_\pi(S) - \hat v(S,w))^2]</script><p>我们目标就是找到最好的w使得<script type="math/tex">J(w)</script> 最小化。</p>
<p>求解期望常见的两种方法：<br>uniform distribution 平均分布：<br>认为每一个状态都是同等重要的。那么每一个状态的权重就是undefinedfrac{1}{|S|}$</p>
<script type="math/tex; mode=display">
J(w) = \mathbb E[(v_\pi(S) - \hat v(S,w))^2] = \frac{1}{|S|} \underset{s \in S} {\sum} (v_\pi(s) - \hat v (s,w))^2</script><p>使用均匀策略的缺点是，我们将很远处的状态和距离目标近处的状态设置权重一样。导致没有侧重点</p>
<p>第二个概率分布：</p>
<p>stationary distribution：</p>
<p>这是一种Markov下的long-run behavior</p>
<p>让undefined{d_\pi(s)\}_{s\in S}<script type="math/tex">作为在策略</script>\pi<script type="math/tex">下的Markov stationary distribution，通过定义</script>d_\pi(s) \ge 0 \text{ and } \underset{s \in S}{\sum}d_\pi(s) = 1$</p>
<p>于是目标函数objective function可以被写为：</p>
<script type="math/tex; mode=display">
J(w) = \mathbb E[(v_\pi(S) - \hat v(S,w))^2] =  \underset{s \in S} {\sum} d_\pi(s)(v_\pi(s) - \hat v (s,w))^2</script><p>Stationary distribution 也被称为steady-state distribution 或者limit distribution。</p>
<p>怎么求<script type="math/tex">d_\pi(s)</script> ？给出一个很长很长的episode：</p>
<p>我们定义<script type="math/tex">n_\pi(s)</script> 表示s出现的次数。于是我们用频率来近似估计<script type="math/tex">d_\pi(s)</script></p>
<script type="math/tex; mode=display">
d_\pi(s) \approx \frac{n_\pi(s)}{\underset{s' \in S}{\sum}n_\pi(s')}</script><p>我们没必要真正的模拟这个episode并统计次数，可以通过数学公式得到：</p>
<p>最终趋于稳定的<script type="math/tex">d_\pi^T</script> 要满足：</p>
<script type="math/tex; mode=display">
d^T_\pi = d^T_\pi P_\pi</script><p>这里的<script type="math/tex">P_\pi</script> 是一个矩阵，代表从<script type="math/tex">s</script> 到<script type="math/tex">s'</script> 转移的概率<script type="math/tex">p(s'|s)</script></p>
<h3 id="optimization-algorithms"><a href="#optimization-algorithms" class="headerlink" title="optimization algorithms"></a>optimization algorithms</h3><p>目标函数的优化算法(optimization algorithms of objective function)</p>
<p>为了最小化目标函数<script type="math/tex">J(w)</script> ，我们可以使用梯度优化gradient-descent </p>
<script type="math/tex; mode=display">
w_{k+1} = w_k - \alpha_k \nabla_w J(w_k)</script><p>这里的true gradient是:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_wJ(w) &= \nabla_w \mathbb E[(v_\pi(S) - \hat v(S,w))^2] \\ 
&= \mathbb E[\nabla_w(v_\pi(S) - \hat v(S,w))^2] \\
&= 2 \mathbb E[(v_\pi(S) - \hat v (S,w))(-\nabla_w \hat v (S,w))] \\
&= -2 \mathbb E [(v_\pi(S) - \hat v(S,w))\nabla _w \hat v(S,w)]
\end{aligned}</script><p>但是求期望很麻烦，所以我们用sotcastic gradient descent来替代gradient descent</p>
<script type="math/tex; mode=display">
w_{t+1} = w_t + \alpha_k(v_\pi(s_t) - \hat v(s_t,wt))\nabla_w \hat v(s_t,w_t)</script><p>但是在现实中，我们是无法得知<script type="math/tex">v_\pi(s_t)</script> 的。</p>
<p>有如下方法：</p>
<ol>
<li><p>Monte Carlo learning 和 value function approximation结合</p>
<p> 让<script type="math/tex">g_t</script> 表示从<script type="math/tex">s_t</script> 开始的episode的return。那么</p>
<p> <script type="math/tex">w_{t+1} = w_t + \alpha_t(g_t-\hat v(s_t,w_t)) \nabla_w \hat v (s_t,w_t)</script></p>
</li>
<li><p>TD learning 和value function approximation结合</p>
<p> 在TD算法中可以用<script type="math/tex">r_{t+1}+\gamma \hat v (s_{t+1},w_t)</script> 来作为<script type="math/tex">v_\pi(s_t)</script> 的一个估计值。于是有</p>
<p> <script type="math/tex">w_{t+1} = w_t + \alpha_t[r_{t+1}+\gamma \hat v (s_{t+1},w_t) - \hat v(s_t,w_t)] \nabla_w \hat v(s_t,w_t)</script></p>
</li>
</ol>
<p>目前只能用来估计给定策略的state values。</p>
<h3 id="selection-of-function-approximators"><a href="#selection-of-function-approximators" class="headerlink" title="selection of function approximators"></a>selection of function approximators</h3><p>函数的选取方法</p>
<ol>
<li><p>选择线性函数（之前广为使用）：</p>
<script type="math/tex; mode=display">
 \hat v (s,w) = \phi^T(s)w</script><p> 此处的undefinedphi(s)$ 是特征向量，他是基于多项式的(polynomial basis),基于傅里叶的(Fourier basis),…</p>
</li>
<li><p>选择神经网络（现在广为使用）：</p>
<p> 网络的参数是w，神经网络的输入是state，输出是估计值undefinedhat v (x,w)$</p>
</li>
</ol>
<p>考虑线性函数：undefinedhat v (s,w) = \phi^T(s)w$ ,我们有</p>
<script type="math/tex; mode=display">
\nabla _w \hat v(s,w) = \phi(s)</script><p>代入TD算法可以得到TD-Linear：</p>
<script type="math/tex; mode=display">
w_{t+1} = w_t +\alpha_t[r_{t+1} + \gamma \phi^T(s_{t+1})w_t - \phi^T(s_t)w_t]\phi(s_t)</script><blockquote>
<p>Linear function approximation的劣势：、</p>
<ul>
<li>很难去选择一个合适的feature vectors</li>
</ul>
<p>Linear function approximation的优势：</p>
<ul>
<li>数学原理清晰，能够可以帮助我们更透彻地研究。</li>
<li>表征能力还算可以。表格形式tabular representation是Linear function approximation 的特殊形式。</li>
</ul>
</blockquote>
<p>Tabular representation 是Linear funciton的一种特殊情况：</p>
<p>首先考虑如下的特征向量：</p>
<script type="math/tex; mode=display">
\phi(s) = e_s \in \mathbb R ^{|S|}</script><p>这里的<script type="math/tex">e_s</script> 是一个只有一个1，其他都是0的向量。</p>
<p>那么这样的话undefinedhat v(s,w) = e_s^T w = w(s)<script type="math/tex">, 这样</script>w(s)$ 就是w的第s个元素了。也就是tabular representation。</p>
<p>然后我们把undefinedphi(s_t) = e_s$ 带入到TD-Linear中。</p>
<script type="math/tex; mode=display">
w_{t+1} = w_t + \alpha(r_{t+1} + \gamma w_t(s_{t+1})-w_t(s_t))e_{S_t}</script><p> 因为<script type="math/tex">e_{s_t}</script> 只有在<script type="math/tex">w_t</script> 的位置是1，其他位置是0，所以在<script type="math/tex">w_t</script> 中只有<script type="math/tex">S_t</script>的位置被更新了。</p>
<script type="math/tex; mode=display">
w_{t+1}(s_t) = w_t(s_t) + \alpha_t(r_{t+1}+\gamma w_t(s_{t+1})-w_t(s_t))</script><p>于是我们发现这个式子和之前tabular的TD算法是一样的。</p>
<h3 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h3><p>Sarsa和value function estimation结合：</p>
<script type="math/tex; mode=display">
\textcolor{ #0000FF}{w_{t+1}} = \textcolor{ #0000FF}{w_t} + \alpha_t[r_{t+1} + \gamma \hat q(s_{t+1},a_{t+1},w_t) - \textcolor{ #0000FF}{\hat  q }(s_t,a_t,w_t)] \textcolor{ #0000FF}{\nabla _w \hat q(s_t,a_t,w_t)}</script><p>除了标蓝的地方略有差别，其他和tabular是一样的。</p>
<blockquote>
<ol>
<li>因为我们描述的参数从state变为了parameter, 所以更新的内容从<script type="math/tex">q(s,a)</script>变为了<script type="math/tex">w</script></li>
<li>使用估计值来估计一个点的state，所以原先的<script type="math/tex">q(s,a)</script> 变为了由函数估计产生的undefinedhat q(s,a,w)$</li>
<li>最后乘上undefinedhat q$ 的梯度来使得向零点移动。</li>
</ol>
</blockquote>
<p>步骤如下：</p>
<ol>
<li><p>对每个episode 执行如下操作：</p>
</li>
<li><p>遵循undefinedpi_t(s_t)<script type="math/tex">执行动作</script>a_t<script type="math/tex">，然后生成</script>r_{t+1},s_{t+1}<script type="math/tex">，然后遵循</script>\pi_t(s_{t+1})<script type="math/tex">执行</script>a_{t+1}$ . </p>
</li>
<li><p>更新值（parameter update）：</p>
<script type="math/tex; mode=display">
 \textcolor{ #0000FF}{w_{t+1}} = \textcolor{ #0000FF}{w_t} + \alpha_t[r_{t+1} + \gamma \hat q(s_{t+1},a_{t+1},w_t) - \textcolor{ #0000FF}{\hat  q }(s_t,a_t,w_t)] \textcolor{ #0000FF}{\nabla _w \hat q(s_t,a_t,w_t)}</script></li>
<li><p>更新策略(policy update):</p>
</li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
\pi_{t+1}(a|s_t) &= 1 - \frac{\epsilon}{|\Alpha(s)|}(|\Alpha(s)|-1) & if a = arg\ max_{a \in \Alpha(s_t)}\textcolor{ #0000FF}{ \hat q(s_t,a,w_{t+1})} \\
\pi_{t+1}(a|s_t) &= \frac{\epsilon}{|\Alpha(s)|} & otherwise
\end{aligned}</script><h3 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h3><p>Q-learning 和value function estimation 结合：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\textcolor{ #0000FF}{w_{t+1}} = \textcolor{ #0000FF}{w_{t}} + \alpha_t[r_{t+1} + \gamma \underset{\alpha \in \Alpha(s_{t+1})}{max}\ \textcolor{ #0000FF}{\hat q}(s_{t+1},a,\textcolor{ #0000FF}{w_t}) - \textcolor{ #0000FF}{\hat q}(s_{t},a,\textcolor{ #0000FF}{w_t})]\textcolor{ #0000FF}{\nabla_w \hat q (s_t,a_t,w_t)}
\end{aligned}</script><p>蓝色部分为value function estimation 版本的Q-learning和tabular 版本之间的区别。</p>
<p>步骤如下（on-policy）：</p>
<ol>
<li><p>对每个episode 执行如下操作：</p>
</li>
<li><p>遵循undefinedpi_t(s_t)<script type="math/tex">执行动作</script>a_t<script type="math/tex">，然后生成</script>r_{t+1},s_{t+1}$ </p>
</li>
<li><p>更新值（parameter update）：</p>
<script type="math/tex; mode=display">
 \begin{aligned}
 \textcolor{ #0000FF}{w_{t+1}} = \textcolor{ #0000FF}{w_{t}} + \alpha_t[r_{t+1} + \gamma \underset{\alpha \in \Alpha(s_t+1)}{max}\ \textcolor{ #0000FF}{\hat q}(s_{t+1},a,\textcolor{ #0000FF}{w_t}) - \textcolor{ #0000FF}{\hat q}(s_{t},a,\textcolor{ #0000FF}{w_t})]\textcolor{ #0000FF}{\nabla_w \hat q (s_t,a_t,w_t)}
 \end{aligned}</script></li>
<li><p>更新策略(policy update)：</p>
<script type="math/tex; mode=display">
 \begin{aligned}
 \pi_{t+1}(a|s_t) &= 1 - \frac{\epsilon}{|\Alpha(s)|}(|\Alpha(s)|-1) & if a = arg\ max_{a \in \Alpha(s_t)}\textcolor{ #0000FF}{ \hat q(s_t,a,w_{t+1})} \\
 \pi_{t+1}(a|s_t) &= \frac{\epsilon}{|\Alpha(s)|} & otherwise
 \end{aligned}</script></li>
</ol>
<h3 id="Deep-Q-learning-deep-Q-network"><a href="#Deep-Q-learning-deep-Q-network" class="headerlink" title="Deep Q-learning(deep Q-network)"></a>Deep Q-learning(deep Q-network)</h3><p>首先定义损失函数：objective function/loss function:</p>
<script type="math/tex; mode=display">
J(w) = \mathbb E [(R+ \gamma \underset{\alpha \in \Alpha(S')}{max} \hat q(S',a,w) - \hat q(S,A,w))^2]</script><p>这是一个贝尔曼最优误差，下式为Q-learning的Bellman optimality error：</p>
<script type="math/tex; mode=display">
q(s,a) = \mathbb E[R_{t+1} + \gamma \underset{\alpha \in \Alpha(S-{t+1})}{max} q(S_{t+1},a) | S_t = s,A_t = a], \forall s,a</script><p>因此<script type="math/tex">R+ \gamma \underset{\alpha \in \mathcal A(S')}{max} \hat q(S',a,w) - \hat q(S,A,w)</script> 的期望应该是0.</p>
<p>如何最小化这个损失函数？ 使用梯度下降法 Grradient-descent：</p>
<p>求<script type="math/tex">J(w)</script> 关于w的梯度，难点在于表达式中由两个地方出现了<script type="math/tex">w</script> ，于是基本思想为我们把前半部分当作一个常数y：</p>
<script type="math/tex; mode=display">
y = R+ \gamma \underset{\alpha \in \Alpha(S')}{max} \hat q(S',a,w)</script><p>这样就只有undefinedhat q(S,A,w)$ 项包含w了，就可以比较简单地求解梯度。</p>
<p>求解方法：</p>
<p>引入两个网络</p>
<ul>
<li>main network 表示undefinedhat q(s,a,w)$  </li>
<li>target network undefinedhat q(s,a,w_T)$</li>
</ul>
<p>将损失函数改写为：</p>
<script type="math/tex; mode=display">
J(w) = \mathbb E [(R+ \gamma \underset{\alpha \in \Alpha(S')}{max} \textcolor{ #FF0000}{\hat q(S',a,w_T)} - \textcolor{ #0000FF}{\hat q(S,A,w)})^2]</script><p>即我们保持target network一段时间不动，因此就能将前半部分作为常数，然后来更新main network。 之后再更新target network。这样也能保证两个网络最后都收敛。</p>
<h3 id="DQN-Experience-replay"><a href="#DQN-Experience-replay" class="headerlink" title="DQN-Experience replay"></a>DQN-Experience replay</h3><p>Experience replay经验回放 ，具体为：</p>
<ul>
<li>我们在收集完数据后，并不根据他们被收集的顺序来使用他们。</li>
<li>而是我们把他们存储到一个集合replay buffer 中， undefinedmathcal B = \{ (s,a,r,s’) \}$</li>
<li>每次训练神经网络的时候， 把他们混到一起，然后取出一些样本(mini-batch)来进行训练。</li>
<li>在取出样本的时候一定要服从均匀分布(uniform distribution)</li>
</ul>
<p>为什么需要经验回放？为什么必须服从均匀分布？</p>
<p>我们观察损失函数：</p>
<script type="math/tex; mode=display">
J(w) = \mathbb E [(R+ \gamma \underset{\alpha \in \Alpha(S')}{max} \hat q(S',a,w) - \hat q(S,A,w))^2]</script><ul>
<li>undefinedS,A) \sim d$ 我们根据索引(S,A) 就能够找到一个唯一的随机变量d</li>
<li><script type="math/tex">R \sim p(R|S,A) ,S' \sim p(S'|S,A)</script> ， R和S’ 由给定的模型决定。</li>
<li>在数据采集的时候，我们可能并不是按照均匀分布采样的。因为他们被确定的策略产生。</li>
<li>为了打破连续样本之间的相关性(通常他们有很强的时间相关性)，我们可以从replay buffer中进行随机均匀采样。</li>
<li>这就是为什么经验回放是必须的，并且是必须均匀分布采样的。</li>
</ul>
<h3 id="Deep-Q-learning"><a href="#Deep-Q-learning" class="headerlink" title="Deep Q-learning"></a>Deep Q-learning</h3><p>off-policy version:<br>目标是从通过 behavior policy undefinedpi_b$ 生成的一些经验中学习一个优化的target network来逼近最优action values 。</p>
<p>步骤如下：</p>
<ol>
<li>存储由behavior policy 生成的经验，存放到replay buffer中 undefinedmathcal B = \{(s,a,r,s’)\}$</li>
<li>对每一次迭代重复如下动作：</li>
<li>从undefinedmathcal B$ 中均匀提取一些样本(mini-batch)</li>
<li>对于每个样本undefineds,a,r,s’)<script type="math/tex">,计算target value</script>y_T = r+ \gamma\ max_{\alpha \in \mathcal A(s’)}(\hat q ,a,w_T)<script type="math/tex">, 在这之中</script>w_T$ 是target network(两个网络之一) </li>
<li>使用mini-batchundefined{(s,a,y_T)\}<script type="math/tex">更新main network 来最小化</script>(y_T - \hat q (s,a,w))^2$ </li>
<li>每进行C次迭代，更新target network：<script type="math/tex">w_T = w</script></li>
</ol>
<h2 id="第九章-Policy-Function-Approximation"><a href="#第九章-Policy-Function-Approximation" class="headerlink" title="第九章 Policy Function Approximation"></a>第九章 Policy Function Approximation</h2><p>policy function approximation 也叫policy gradient</p>
<p>之前的方法都是value based ，本次的算法是policy based</p>
<p>在这之前的策略都是用表格来表达的： 即给定undefineds_i,a_j)<script type="math/tex">，会得到一个策略</script>\pi(a_i|s_j)$ 。</p>
<p>我们将他写成函数</p>
<script type="math/tex; mode=display">
\pi(a|s,\theta)</script><p>这里的undefinedtheta$ 是一个向量，表示参数。</p>
<blockquote>
<p>用函数代替表格的好处：</p>
<ul>
<li>如果state有很多很多个，那么在存储上会很费力，</li>
<li>难以进行泛化，在表格中，如果要更改undefinedpi(a|s)<script type="math/tex">，那么一定要访问</script>\pi(a|s)<script type="math/tex">，而如果用表达式来表达的话，则可以通过更改参数来更改一系列的</script>\pi$  </li>
</ul>
</blockquote>
<p>tabular 和function representations的区别：</p>
<ol>
<li><p>定义最优的策略：</p>
<p> 在表格情况下，策略undefinedpi$ 当在每一个state value上都最大的时候是最优的。</p>
<p> 在函数表示中，策略undefinedpi$ 当能够最大化scalar metrics的时候是最优的</p>
</li>
<li><p>怎么获取一个action的probability？</p>
<p> 在表格中，使用索引来得到一个action的probability。</p>
<p> 在函数中，需要放入神经网络中进行计算得出。</p>
</li>
<li><p>如何更新policies？</p>
<p> 在表格中：直接改变表格中的undefinedpi(a|s)$ 的值</p>
<p> 在函数中，通过更改undefinedtheta$ 间接修改策略。</p>
</li>
</ol>
<p>policy gradient的目标函数(metric)是：最大化 <script type="math/tex">J(\theta)</script></p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t + \alpha \nabla _\theta J(\theta_t)</script><h3 id="policy-gradient的两个metrics"><a href="#policy-gradient的两个metrics" class="headerlink" title="policy gradient的两个metrics"></a>policy gradient的两个metrics</h3><p>metric（度量）</p>
<ol>
<li>第一个metric是state value的加权平均。</li>
</ol>
<script type="math/tex; mode=display">
\overline v_\pi = \underset{s \in S} {\sum} d(s) v_\pi(s) = d^Tv_\pi</script><p>在这里undefinedsum_{s \in S} d(s) = 1<script type="math/tex">，既可以代表权重，可以代表选择</script>v_\pi(s)$ 的概率。</p>
<p>如何选择d(s)？</p>
<p>第一种情况是d和undefinedpi$ 无关。</p>
<ul>
<li><p>那么求undefinedoverline v_\pi<script type="math/tex">的梯度不需要得到d对</script>\pi$ 的梯度。</p>
</li>
<li><p>例如令<script type="math/tex">d_0(s) = \frac{1}{|S|}</script> ,得到均匀分布。</p>
</li>
<li>或者我们对其中的某些状态很关心<script type="math/tex">d_0(s_0) = 1,d_0(s \not = s_0) = 0</script> ，这时候undefinedoverline v_\pi = v_\pi(s_0)$</li>
</ul>
<p>第二种情况是d和undefinedpi<script type="math/tex">有关,即d依赖于</script>\pi$ </p>
<ul>
<li>一种基本的情况是<script type="math/tex">d_\pi</script>满足<script type="math/tex">d^T_\pi P_\pi = d^T_\pi</script> </li>
<li>那么会有些状态访问的次数较多，有些状态访问的次数较少。</li>
</ul>
<ol>
<li>第二个metric是average one-step reward</li>
</ol>
<script type="math/tex; mode=display">
r_\pi \approx \underset{s \in S }{\sum}d_\pi(s)r_\pi(s) = \mathbb [r_\pi(S)]</script><p>在这里<script type="math/tex">S \sim d_\pi</script>  ,有如下公式：</p>
<script type="math/tex; mode=display">
r_\pi(s) \approx \underset{a \in \mathcal A}{\sum} \pi(a|s)r(s,a)</script><p>代表在一个状态得到的reward的加权平均。</p>
<script type="math/tex; mode=display">
r(s,a) = \mathbb E(R|s,a) = \underset{r} \sum rp(r|s,a)</script><p>下面给出average one-step reward的第二种形式：</p>
<p>假设根据一个给定的策略生成了一个trajectory，并有着undefinedR_{t+1},R_{t+2},R_{t+3},…)$ 的reward。</p>
<p>那么在这个trajectory中，平均的single-step reward是</p>
<script type="math/tex; mode=display">
\begin{aligned}
& \underset{n \to \infty} {lim} \frac{1}{n} \mathbb E[R_{t+1}+R_{t+2}+...+R_{t+n}|S_t = s_0] \\
= &\underset{n \to \infty} {lim} \frac{1}{n} \mathbb E[\underset{k=1}{\overset{n}{\sum}}R_{t+k}|S_t = s_0] = \underset{n \to \infty} {lim} \frac{1}{n} \mathbb E[\underset{k=1}{\overset{n}{\sum}}R_{t+k}]
\end{aligned}</script><p>注意到最后当<script type="math/tex">n \to \infty</script> 时，可以将<script type="math/tex">S_t=s_0</script> 省去，因为当走了无穷步的时候，从哪一步开始就已经无所谓了。</p>
<p>注意点1：<br>所有这些的metrics都是undefinedpi<script type="math/tex">的函数。因为</script>\pi<script type="math/tex">是由参数</script>\theta<script type="math/tex">决定，这些metrics也是关于</script>\theta<script type="math/tex">的函数，也就是说不同的</script>\theta<script type="math/tex">会产生不同的metric values. 我们就可以找到最优的</script>\theta$ 来使得这些metrics最优。</p>
<p>注意点2：</p>
<p>这些metrics可以被定义为有折旧的情况(discounted case)，即undefinedgamma \in [0,1)<script type="math/tex">，也可以是undiscounted case的，即</script>\gamma = 1$ </p>
<p>注意点3：</p>
<p>在直观上，undefinedoverline r_\pi<script type="math/tex">和</script>\overline v_\pi<script type="math/tex">相比，似乎更加”近视“，因为他更多地考虑立即的reward，而</script>\overline v_\pi$ 考虑整个过程的reward。 </p>
<p>实则不然，两个metrics实际上是相互等价的。当undefinedgamma &lt; 1$ ，有如下公式：</p>
<script type="math/tex; mode=display">
\overline r_\pi = ( 1- \gamma) (\overline v_\pi)</script><p>metric还有一种常见的表示形式如下：</p>
<script type="math/tex; mode=display">
J(\theta) = \mathbb E[\underset{i = 0}{\overset{\infty}{\sum}}\gamma ^t R_{t+1}]</script><p>它实际上和undefinedoverline v_\pi$ 是相同的，下面给出推导过程：</p>
<script type="math/tex; mode=display">
\begin{aligned}
J(\theta) &= \mathbb E[\underset{i = 0}{\overset{\infty}{\sum}}(\gamma ^t R_{t+1})] \\
&=\mathbb E[\underset{i = 0}{\overset{\infty}{\sum}}(\gamma ^t \underset{s \in S}{\sum}d(s)R_{t+1,s})] \\
& = \underset{s \in S}{\sum} (d(s) \mathbb E[\underset{t = 0}{\overset{\infty}{\sum}}\gamma ^t R_{t+1}|S_0 = s] )\\
& = \underset{s \in S}{\sum} d(s) v_\pi(s) =\overline v_\pi
\end{aligned}</script><h3 id="目标函数的梯度计算"><a href="#目标函数的梯度计算" class="headerlink" title="目标函数的梯度计算"></a>目标函数的梯度计算</h3><p>梯度如下：</p>
<script type="math/tex; mode=display">
\nabla _\theta J(\theta) = \underset{s \in S } {\sum } \eta(s)\underset{a \in \mathcal A} \sum \nabla _\theta \pi(a|s,\theta) q_\pi(s,a)</script><p>又可以写作</p>
<script type="math/tex; mode=display">
\nabla _\theta J(\theta) =  \mathbb E[\nabla_\theta ln\ \pi(A|S,\theta)q_\pi(S,A)]</script><p>写成期望的形式便于我们用采样来模拟期望。</p>
<p>下面是推导期望公式的过程：</p>
<p>由链式法则得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_\theta ln \pi(a|s,\theta) &= \frac{\nabla_\theta \pi(a|s,\theta)}{\pi(a|s,\theta)} \\
\nabla _\theta \pi (a|s,\theta) &= \pi(a|s,\theta) \nabla_\theta ln\pi(a|s,\theta)
\end{aligned}</script><p>把式子带入梯度表达式中：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla _\theta J(\theta) &= \underset{s } {\sum } d(s)\underset{a} \sum \textcolor{ #0000FF}{\nabla _\theta \pi(a|s,\theta)} q_\pi(s,a) \\
& = \underset{s}{\sum} d(s) \underset{a}\sum \textcolor{ #000FFF}{\pi(a|s,\theta) \nabla_\theta ln \pi(a|s,\theta)} q_\pi(s,a) \\
& = \mathbb E _{\textcolor{ #000FFF}{S\sim d}} [\underset{a}{\sum}\pi(a|S,\theta)q_\pi(S,A)] \\ 
& = \mathbb E_{S \sim d,\textcolor{ #000FFF}{A \sim \pi}}[\nabla_\theta ln \pi(A|S,\theta) q_\pi(S,A)] \\
&= \mathbb E[\nabla_\theta ln \pi(A|S,\theta) q_\pi(S,A)] 
\end{aligned}</script><p>因为我们求了<script type="math/tex">ln(\pi(a|s,\theta))</script> 所以我们的undefinedpi<script type="math/tex">必须满足</script>\pi(a|s,\theta) &gt; 0<script type="math/tex">,可以用softmax方式来将</script>(-\infty ,\infty)<script type="math/tex">的值域归一化到</script>(0,1)$</p>
<script type="math/tex; mode=display">
z_i = \frac{e^{x_i}}{\sum^b_{j=1}e^{x_j}}</script><p>同时还满足undefinedsum_{i=1}^b z_i = 1$</p>
<h3 id="REINFORCE"><a href="#REINFORCE" class="headerlink" title="REINFORCE"></a>REINFORCE</h3><p>reinforce是一种 policy gradient algorithm</p>
<p>根据上一节的目标函数，我们可以得到迭代方程：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{t+1} &= \theta_t +\alpha_\theta J(\theta) \\
 &= \theta_t +\alpha_\theta \mathbb E[\nabla_\theta ln \pi(A|S,\theta) q_\pi(S,A)] \\
\end{aligned}</script><p>但我们知道，期望是很难算的， 我们要用stochastic方式来代替真实的期望。</p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t +\alpha_\theta \mathbb \nabla_\theta ln \pi(a_t|s_t,\theta_t) q_\pi(s_t,a_t)</script><p>但我们的<script type="math/tex">q_\pi</script> 是不知道的，于是我们可以用<script type="math/tex">q_t</script> 来代替<script type="math/tex">q_\pi</script></p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t + \alpha \nabla_\theta ln \pi(a_t | s_t,\theta_t)\textcolor{ #0000FF}{q_t(s_t,a_t)}</script><p>如何对<script type="math/tex">q_t</script>进行采样？</p>
<ol>
<li><p>基于蒙德卡罗的方法，即本节的REINFORCE</p>
</li>
<li><p>其他方法</p>
</li>
</ol>
<p>注意一：</p>
<p>如何做采样？</p>
<ol>
<li>如何对S做采样？<script type="math/tex">S \sim d</script> ，经过undefinedpi<script type="math/tex">下不断迭代得到</script>d$ ,然后采样得到S。但实际中我们没时间等d趋于平稳再采样。</li>
<li>如何对A做采样？ <script type="math/tex">A \sim \pi(A|S,\theta)</script> , 根据 在<script type="math/tex">s_t</script> 处的策略undefinedpi(\theta_t)$ 来对A进行采样。</li>
</ol>
<p>因此，这个policy gradient 方法是on-policy的。</p>
<p>注意二：<br>如何理解算法？</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{t+1} &= \theta_t \alpha \nabla_\theta ln \pi(a_t|s_t,\theta_t) q_t(s_t,a_t) \\ 
&= \theta_t + \alpha(\frac{q_t(s_t,a_t)}{\pi(a_t|s_t,\theta_t)}) \nabla _\theta \pi(a_t|s_t,\theta_t)
\end{aligned}</script><p>我们设定undefinedbeta_t = \frac{q_t(s_t,a_t)}{\pi(a_t|s_t,\theta_t)}$ </p>
<p>于是原式变为如下：</p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t + \alpha \beta_t \nabla _\theta \pi(a_t|s_t,\theta_t)</script><p>于是我们就可以发现，我们是通过改变undefinedtheta<script type="math/tex">，来优化</script>\pi(a_t|s_t,\theta)$ 的值。</p>
<p>所以我们步长undefinedalpha \beta_t$ 要足够小才能收敛。</p>
<ul>
<li>当undefinedbeta_t &gt; 0<script type="math/tex">, 那么选择</script>(s_t,a_t) <script type="math/tex">的可能更大，于是有</script>\pi(a_t |s_t,\theta_{t+1}) &gt; \pi(a_t|s_t,\theta_t)$</li>
<li>当undefinedbeta_t &lt; 0<script type="math/tex">，那么</script>\pi(a_t|s_t,\theta_{t+1}) &lt; \pi(a_t|s_t,\theta_t)$ </li>
</ul>
<p>数学推导如下：</p>
<p>当undefinedtheta_{t+1} - \theta_t$ 足够小时，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi(a_t|s_t ,\theta_{t+1}) &\approx \pi(a_t|s_t,\theta_t) + (\nabla_\theta \pi(a_t|s_t ,\theta_t))^T(\theta_{t+1} - \theta_t) \\
 &= \pi(a_t|s_t,\theta_t) + \alpha \beta_t (\nabla_\theta \pi(a_t|s_t,\theta_t))^T(\nabla_\theta \pi(a_t|s_t,\theta_t))\\
 &= \pi(a_t|s_t,\theta_t) + \alpha \beta_t ||\nabla_\theta \pi(a_t|s_t,\theta_t)||^2
\end{aligned}</script><p>系数undefinedbeta_t<script type="math/tex">能够很好地平衡</script>exploration<script type="math/tex">和</script>exploitation$</p>
<p>首先undefinedbeta_t<script type="math/tex">与</script>q_t(s_t,a_t)$ 成正比。</p>
<ul>
<li>如果<script type="math/tex">q_t(s_t,a_t)</script> 比较大，那么undefinedbeta_t$ 也比较大。</li>
<li>因此此时算法倾向于使用更大的值来增强<script type="math/tex">q_t</script> ,也就是exploitation</li>
</ul>
<p>其次：undefinedbeta_t<script type="math/tex">与</script>\pi(a_t|s_t,\theta_t)$ 成反比。</p>
<ul>
<li><p>如果undefinedpi(a_t|s_t,\theta_t)<script type="math/tex">很小，那么</script>\beta_t<script type="math/tex">很大。下次的</script>pi(a_t|s_t,\theta_{t+1})$更大，就会给出更大的选择概率。</p>
</li>
<li><p>因此算法倾向于概率较低的探索操作。即exploration</p>
</li>
</ul>
<p>REINFORCE算法实现如下；</p>
<p>初始化参数undefinedpi(a|s,\theta), \gamma \in (0,1) ,\alpha &gt; 0$</p>
<p>目标是最大化<script type="math/tex">J(\theta)</script></p>
<p>对于第k次迭代：</p>
<ol>
<li>选择一个<script type="math/tex">s_0</script> ，遵循undefinedpi(\theta_k)<script type="math/tex">生成episode ，假设episode是</script>\{s_0,a_0,r_1,…,s_{T-1},a_{T-1},r_{T}\}$</li>
<li>对于每个<script type="math/tex">t = 0,1,2,.,,,T-1</script> 执行3和4：</li>
<li>value update：<script type="math/tex">q_t(s_t,a_t) = \sum_{k=t+1}^T \gamma^{k-t-1}r_k</script></li>
<li>policy update:undefinedtheta_{t+1} = \theta_t + \alpha \nabla_\theta ln \pi(a_t|s_t,\theta_t)q_t(s_t,a_t)$</li>
</ol>
<h2 id="第十章-actor-critic-方法"><a href="#第十章-actor-critic-方法" class="headerlink" title="第十章 actor-critic 方法"></a>第十章 actor-critic 方法</h2><p>actor-critic本身就是policy gradient</p>
<h3 id="The-simplest-actor-critic"><a href="#The-simplest-actor-critic" class="headerlink" title="The simplest actor-critic"></a>The simplest actor-critic</h3><p>也称QAC（这里的Q是公式中的q，也就是action value）</p>
<p>policy gradient算法：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{t + 1} &= \theta  + \alpha \nabla_\theta J(\theta_t) \\
&= \theta _t + \alpha \mathbb E_{S \sim \eta,A \sim \pi}[\nabla _\theta ln \pi(A|S,\theta_t)q_\pi(S,A)] \\
\theta_{t+1} &= \theta_t + \alpha \nabla_\theta ln \pi(a_t|s_t,\theta_t) q_t(s_t,a_t)
\end{aligned}</script><p>这个更新策略的算法就是actor， critic则用来估计<script type="math/tex">q_t(s_t,a_t)</script></p>
<p>如何得到<script type="math/tex">q_t(s_t,a_t)</script> ？ </p>
<p>两种方法：</p>
<ol>
<li>MC learning：这样结合就得到了REINFORCE算法。</li>
<li>Temporal-difference learning： actor-critic算法。</li>
</ol>
<p>优化目标函数<script type="math/tex">J(\theta)</script> ，使其最大化。</p>
<p>对于每个episode的第t步，执行如下：</p>
<ol>
<li><p>遵循undefinedpi(a|s_t,\theta_t)<script type="math/tex">生成</script>a_t<script type="math/tex">，得到(</script>r_{t+1},s_{t+1}undefined ,然后遵循undefinedpi (a|s_{t+1},\theta_t)<script type="math/tex">生成</script>a_{t+1}$</p>
</li>
<li><p>Critic（value update）：</p>
<script type="math/tex; mode=display">
 w_{t+1} = w_t + \alpha_w [r_{t+1} + \gamma q(s_{t+1},a_{t+1}),w_t] - q(s_t,a_t,w_t) \nabla_w q(s_t,a_t,w_t)</script><p> Actor (policy update):</p>
<script type="math/tex; mode=display">
 \theta_{t+1} = \theta_t + \alpha_\theta \nabla_{\theta} ln \pi (a_t|s_t,\theta_t) q(s_t,a_t,w_{t+1})</script></li>
</ol>
<p>这个算法是on-policy 的。</p>
<p>The simplest actor-critic实际上就是 SARSA + value function approximation</p>
<h3 id="advantage-actor-critic"><a href="#advantage-actor-critic" class="headerlink" title="advantage actor-critic"></a>advantage actor-critic</h3><p>也叫AAC ，A2C</p>
<p>首先我们为policy gradient 引入一个新的baseline（b函数）</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla _\theta J(\theta) &= \mathbb E_{S \sim \eta ,A \sim \pi } [\nabla_\theta \ln \pi (A|S,\theta_t) q_\pi(S,A)] \\
&= \mathbb E_{S \sim \eta ,A \sim \pi } [\nabla_\theta \ln \pi (A|S,\theta_t) (q_\pi(S,A) - \textcolor{ #0000FF}{b(S)})] 
\end{aligned}</script><p>为什么引入新的b 函数，等式依然成立？</p>
<p>因为如下公式成立：</p>
<script type="math/tex; mode=display">
\mathbb E_{S \sim \eta ,A \sim \pi } [\nabla_\theta \ln \pi (A|S,\theta_t)b(S)] = 0</script><p>详细地说:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb E_{S \sim \eta ,A \sim \pi } [\nabla_\theta \ln \pi (A|S,\theta_t)b(S)] &= \underset{s \in S}{\sum} \eta(s) \underset{a \in \mathcal A}{\sum} \pi(a|s,\theta_t) \nabla_\theta \ln\pi(a|s,\theta_t) b(s) \\ 
 &= \underset{s \in S}{\sum} \eta(s) \underset{a \in \mathcal A}{\sum} \nabla_\theta \pi (a|s,\theta_t) b(s) \\
 &= \underset{s \in S}{\sum} \eta(s)  b(s) \underset{a \in \mathcal A}{\sum} \nabla_\theta \pi(a|s,\theta_t) \\
 &=\underset{s \in S}{\sum} \eta(s)  b(s) \nabla_\theta \underset{a \in \mathcal A}{\sum}  \pi(a|s,\theta_t) \\
 &= \underset{s \in S}{\sum} \eta(s)  b(s) \nabla_\theta 1 =0
 \end{aligned}</script><p>引入这个b函数有什么用？</p>
<p>我们说undefinednabla_\theta J(\theta) = \mathbb E[X]$</p>
<p>那么我们知道</p>
<ul>
<li>undefinedmathbb E[X]$ 和b(S) 无关。</li>
<li>X的方差和b有关。</li>
</ul>
<p>所以我们可以通过设置b函数来减小方差。</p>
<p>设置b函数为如下值时，能使得方差最小：</p>
<script type="math/tex; mode=display">
b^* (s) = \frac{\mathbb E_{A\sim \pi }[||\nabla_\theta \ln \pi (A|s,\theta_t)||^2 q(s,A)||]}{\mathbb E_{A\sim \pi }[||\nabla_\theta \ln \pi (A|s,\theta_t)||^2||]}</script><p>其中<script type="math/tex">||\nabla_\theta \ln \pi (A|s,\theta_t)||^2</script> 可以被认为是一个权重。</p>
<p>但是这个公式太复杂了。我们一般直接用</p>
<p><script type="math/tex">b(s) = \mathbb E_{A \sim \pi}[q(s,A)] = v_\pi(s)</script></p>
<p>把上式带入公式中，我们可以得到gradient-ascent算法：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{t+1} &= \theta_t + \alpha \mathbb E[\nabla_\theta \ln \pi (A|S,\theta_t) ( \textcolor{ #0000FF}{q_\pi(S,A) - v_\pi(S)})] \\ 
&= \theta_t + \alpha \mathbb E[\nabla_\theta \ln \pi (A|S,\theta_t) ( \textcolor{ #0000FF}{\delta_\pi(S,A)})]
\end{aligned}</script><p>我们叫undefineddelta_\pi(S,A) = q_\pi(S,A) - v_\pi(S)$ 为advantage funciton（优势函数）</p>
<p><script type="math/tex">v_\pi(S)</script> 是某个状态下的action的平均值， 所以undefineddelta_\pi(S,A)$ 描述了当前的action和同状态的其他action相比的优劣。</p>
<p>公式还可以写成下面：</p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta _t + \alpha \nabla_\theta \ln \pi (a_t|s_t,\theta_t) \delta_t(s_t,a_t) \\ 
 = \theta _t + \alpha  \frac{\nabla_\theta\pi (a_t|s_t,\theta_t)}{\pi(a_t|s_t,\theta_t)} \delta_t(s_t,a_t) \\
 = \theta _t + \alpha  \frac{\delta_t(s_t,a_t) }{\pi(a_t|s_t,\theta_t)} \nabla_\theta\pi (a_t|s_t,\theta_t)</script><p>于是我们公式中的undefinedfrac{\delta_t(s_t,a_t) }{\pi(a_t|s_t,\theta_t)}<script type="math/tex">决定了step-size（和第9讲REINFORCE中的</script>\beta_t<script type="math/tex">一样能够很好地平衡</script>exploration<script type="math/tex">和</script>exploitation$</p>
<p>A2C ，或者TD actor-critic  的过程：</p>
<p>目标是寻找最大的<script type="math/tex">J(\theta)</script></p>
<p>在每个episode的第t时刻，我们执行如下：</p>
<ol>
<li><p>遵循undefinedpi(a|s_t,\theta_t)<script type="math/tex">生成</script>a_t<script type="math/tex">然后得到</script>r_{t+1},s_{t+1}$ </p>
</li>
<li><p>TD error(advantage function):</p>
<p> undefineddelta_t = r_{t+1} + \gamma v(s_{t+1},w_t) - v(s_t,w_t)$</p>
</li>
<li><p>Critic (value update):</p>
<p> <script type="math/tex">w_{t+1} = w_t + \alpha_w \delta_t \nabla_w v(s_t,w_t)</script></p>
</li>
<li><p>Actor(plicy update):</p>
<p> undefinedtheta_{t+1} = \theta_t + \alpha_\theta \delta_t \nabla_\theta \ln \pi (a_t|s_t,\theta_t)$</p>
</li>
</ol>
<p>这是一个on-policy 的。</p>
<h3 id="off-policy-actor-critic"><a href="#off-policy-actor-critic" class="headerlink" title="off-policy actor-critic"></a>off-policy actor-critic</h3><p>Policy gradient是on-policy的原因是梯度必须服从undefinedpi<script type="math/tex">策略，这里的</script>\pi<script type="math/tex">既是behavior policy ，同时这个</script>\pi$ 也是我们要更新的target policy。</p>
<p>可以使用importance sampling 来把on-policy转为off-policy。</p>
<script type="math/tex; mode=display">
\mathbb E_{X \sim p_0} [X] = \underset{x}{\sum}p_0(x)x = \underset{x}{\sum} p_1(x) \frac{p_0(x)}{p_1(x)}x = \mathbb E_{X\sim p_1} [f(X)]</script><p>于是我们就可以通过<script type="math/tex">p_1</script> 进行采样，然后估计<script type="math/tex">p_0</script> 采样下的均值。 那么热和计算<script type="math/tex">\mathbb E_{X\sim p_1} [f(X)]</script> ?</p>
<p>令f为如下函数：</p>
<script type="math/tex; mode=display">
f = \frac{1}{n} \underset{i = 1}{\overset{n}{\sum}} f(x_i) , \text{where } x_i \sim p_i</script><p>那么就有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb E_{X \sim p_1}[\overline f] &= \mathbb E _{X \sim p_1} [f(X)] \\
var_{X \sim p _ 1} [\overline f] &= \frac{1}{n} var_{X \sim p _1}[f(X)]
\end{aligned}</script><p>所以undefinedoverline f<script type="math/tex">（f的平均数）就可以用来估计</script>\mathbb E_{X \sim p_1}[\overline f] = \mathbb E _{X \sim p_0} [X] $</p>
<script type="math/tex; mode=display">
\mathbb E_{X \sim p_0} [X] \approx \overline f = \frac{1}{n}\underset{i = 1}{\overset{n}{\sum}} f(x_i) = \frac{1}{n} \underset{i = 1}{\overset{n}{\sum}}\frac{p_0(x_i)}{p_1(x_i) }x_i</script><p>这里的undefinedfrac{p_0(x_i)}{p_1(x_i) }<script type="math/tex">可以被认为是权重，那么直观地看就是对于</script>p_0$ 相对难取的样本，赋予更高的权重。</p>
<p>这个权重叫做 importance权重。</p>
<p>就是因为我们只能知道<script type="math/tex">p_0(x)</script> ，但求不出undefinedmathbb E_{X \sim o_0}[X]$ , 所以才需要importance sampling。</p>
<p> 假设undefinedbeta$ 是behavior policy生成的经验采样。</p>
<p>我们的目标是更新target policy undefinedpi<script type="math/tex">来最大化</script>J(\theta)$</p>
<script type="math/tex; mode=display">
J(\theta) = \underset{s \in S}{\sum} d_\beta(s) v_\pi(s) = \mathbb E _{S \sim d_\beta} [v_\pi (S)]</script><p>他的梯度如下：</p>
<script type="math/tex; mode=display">
\nabla _\theta J(\theta) = \mathbb E_{S \sim \rho,A \sim \beta} [\frac{\pi(A|S,\theta)}{\beta(A|S)} \nabla_\theta \ln \pi(A|S,\theta)q_\pi(S,A)]</script><p>这里的undefinedbeta<script type="math/tex">是behavior policy ，</script>\rho$ 是state distribution。</p>
<p>优化：</p>
<p>我们仍然可以通过加上baseline来进行优化：undefineddelta _\pi(S,A) = q_\pi(S,A) - v_\pi(S) $ 。</p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t + \alpha_\theta \frac{\pi(a_t|s_t,\theta_t)}{\beta(a_t|s_t)} \nabla_\theta \ln \pi(a_t|s_t,\theta_t) (q_t(s_t,a_t) - v_t (s_t))</script><p>在这之中</p>
<script type="math/tex; mode=display">
q_t (s_t,a_t) - v_t(s_t) \approx r_{t+1} + \gamma v_t(s_{t+1}) - v_t(s_t) = \delta_t(s_t,a_t)</script><p>于是最终的算法就是</p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t + \alpha_\theta \frac{\delta_t (s_t,a_t)}{\beta(a_t|s_t)} \nabla_\theta \ln \pi(a_t|s_t,\theta_t) \pi(a_t|s_t,\theta_t)</script><h3 id="Deterministic-actor-critic"><a href="#Deterministic-actor-critic" class="headerlink" title="Deterministic actor-critic"></a>Deterministic actor-critic</h3><p>DPG和之前的（QAC，A2C、off-policy的actor-critic）相比的一大特点就是他的策略undefinedpi(a|s,\theta)$  可以是负数。</p>
<p>于是我们用deterministic policies来解决continuous action（无限个的、连续的action）</p>
<p>之前我们是通过策略 undefinedpi(a|s,\theta) \in [0,1]$ 来决定要采取哪个动作a。</p>
<p>而现在我们改成下面这样：</p>
<script type="math/tex; mode=display">
a = \mu (s,\theta)</script><p> 意味着我们直接通过s得到a的值，而不是借助每一个action的概率来决定选择哪个a。</p>
<script type="math/tex; mode=display">
J(\theta) = \mathbb E [v_\mu (s)] = \underset{s \in S}{\sum} d_0 (s)v_\mu (s)</script><p><script type="math/tex">d_0</script> 的选择和undefinedmu$ 无关。</p>
<p>选择<script type="math/tex">d_0</script>的两种特殊的情况：</p>
<ol>
<li><script type="math/tex">d_0(s_0) - 1</script> ,<script type="math/tex">d_0(s \not = s_0) = 0</script> . 在这里<script type="math/tex">s_0</script> 是一个特殊的开始状态。</li>
<li><script type="math/tex">d_0</script>  取决于behavior policy 在undefinedmu$ 上的内容。</li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_\theta J(\theta) &= \underset{s \in S}{\rho_\mu(s) \nabla_\theta \mu(s)(\nabla_a q_\mu(s,a))|_{a = \mu(s)}} \\
&= \mathbb E_{S \sim \rho_\mu} [\nabla_\theta \mu(s)(\nabla_a q_\mu(s,a))|_{a = \mu(s)}]
\end{aligned}</script><p>这里面的梯度没有action A。</p>
<p>所以这个deterministic policy gradient 是一个off-policy的方法。（因为我们不需要关心这个a是通过哪个策略得到的）</p>
<p>梯度上升：</p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t + \alpha_\theta \mathbb E_{S \sim \rho_\mu} [\nabla_\theta \mu(s)(\nabla_a q_\mu(s,a))|_{a = \mu(s)}] \\
\theta_{t+1} = \theta_t + \alpha_\theta \nabla_\theta \mu(s_t) (\nabla _a q_\mu (s_t,a))|_{a = \mu(s)}</script><p>注意：</p>
<ul>
<li><p>undefinedbeta<script type="math/tex">和</script>\mu$ 是不同的。</p>
</li>
<li><p>undefinedbeta<script type="math/tex">也可以设置为</script>\mu + noiseundefined </p>
</li>
</ul>
<p>如何选取<script type="math/tex">q(s,a,w)</script> ?</p>
<ol>
<li>线性函数： <script type="math/tex">q(s,a,w) = \phi^T(s,a)w</script> </li>
<li>神经网络：DDPG</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="lsk404.github.io">Little_sk</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://lsk404.github.io/887930e5b441/">https://lsk404.github.io/887930e5b441/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a><a class="post-meta__tags" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E5%AD%A6/">数学</a></div><div class="post-share"><div class="social-share" data-image="/img/avator.jpg" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/d49b5a7aa584/" title="2025蓝桥杯PythonA组省赛题解"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">2025蓝桥杯PythonA组省赛题解</div></div><div class="info-2"><div class="info-item-1">2025蓝桥杯PythonA组省赛题解 一定注意，由于在编写本题解时还没有在线题目。所以： 本题解仅供参考，在题意、思路、code上都可能发生错误！  A. RGB三色题意我们可以用三个0~255之间的数(r,g,b)来表示一个颜色，如(0,0,255) 表示蓝色。 那么请问所有的颜色中，有多少种颜色是“偏蓝色” 我们定义当且仅当b > r 并且b > g ，undefinedr,g,b)$ 是偏蓝色的。 思路 &amp; 代码直接暴力枚举！ 复杂度256^3 = 16777216 。 1234567ans = 0for r in range(256):    for g in range(256):        for b in range(256):            if(b &gt; g and b &gt; r):                ans += 1print(ans) # 5559680 方法二： 使用公式计算： 如果我们给定b，那么r和g可以取[0,b-1] 中的任何一个值，有b^2 种可能。于是我们就有如下公式：  ans =...</div></div></div></a><a class="pagination-related" href="/555324d54baf/" title="期末回顾"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">期末回顾</div></div><div class="info-2"><div class="info-item-1">本文知识点均为从网络或其他地方搜集整理而来，提到的习题均无答案，完全不具备权威性质，作者不对看本文造成的后果负责，请合理参考。本人为ZZU2022级网安学院物联考生，以下是我对实际考试的考后记录，仅供复习参考。不保证提供的信息完全正确！！！用爱发电，欢迎纠正和补充，分享时请注明来源 我的复习资料合集[夸克网盘](https://pan.quark.cn/s/282df4876ab0) 资料仅供分享，不要抄袭！转发请注明出处。  另附20级物联大佬做的总结：https://wyqz.top/p/454636061.html  2022大一上下期末回顾大一都是公修课，倒是用不上太多复习资料。 就随便说一下复习策略：  数学相关/离散数学/大学物理：一定要多看课后题，很多原题！！（看不懂就重新回顾知识点） 英语：背单词，没啥多说的。（听力需要重视一下） 政治：平时学不学区别不大，最后一两节课老师很可能会划重点。也可以去网上找资料（仅限政治可以去网上找资料！）  2023大二上期末回顾计算机组成原理任课老师：王银玲老师 ，林楠老师**题型**40选择10判断5计算...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/10a9231f6b6b/" title="ACM笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-17</div><div class="info-item-2">ACM笔记</div></div><div class="info-2"><div class="info-item-1">数论博弈论：sg函数定义**NIM博弈：** 给定n堆物品，第i堆物品有A_i 个，两名玩家轮流行动，每次任选一堆，取出任意多个物品，取走最后一件物品者胜利。 **当且仅当 A_1 xorA_2xor...xorA_n时，NIM博弈先手必胜。** **公平组合游戏ICG**：（NIM就是经典的公平组合游戏） 1.由两名玩家交替行动2.在游戏进程的任意时刻，可以执行的合法行动与轮到哪名玩家无关3.不能行动的玩家判负 **有向图游戏：** 给定一个有向无环图，途中有唯一的起点，在起点放一枚棋子。两名玩家交替把这个棋子沿边移动，无法移动者判负。 **公平组合游戏都可以转化为有向图游戏。** **SG函数**： 在有向图游戏中，对于节点x，设从节点x出发分别到达了y_1,y_2,...,y_k ，定义sg(x): sg(x) = mex(\{ sg(y_1),sg(y_2),...,sg(y_k)\}) 。（mex(s)表示不属于集合s的最小非负整数） 对于整个有向图G，sg(G) = sg(s) 对于多个有向图游戏G_1,G_2,...,G_m的和GSG(G) =...</div></div></div></a><a class="pagination-related" href="/a726c975d078/" title="mysql远程连接"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-29</div><div class="info-item-2">mysql远程连接</div></div><div class="info-2"><div class="info-item-1">搞了一天的mysql远程连接， 也算是踩过坑了，这里记录一下 如果你的mysql无法被远程连接，那么请依次检查如下条件： 1. 服务器的防火墙（安全组）放行端口如果mysql服务在云服务器上运行， 那么请检查云服务器是否将端口3306开放。 如果没有开放请设置为通行状态。。这里不同的云服务器厂商的截图不同，我这里给出华为云的安全组设置 华为云：![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/e44f593c14434d7e87359c45016de270.png#pic_center) 2. 启动mysql的远程连接服务在命令台中使用mysql -uroot 进入你的mysql数据库。1mysql -uroot 选择mysql库1use mysql检查user表中的host和User信息。1select host,User from user;如果你的root字段对应的host是localhost，那么代表root账户只能本地登录。123456789+-----------+------------------+| host   ...</div></div></div></a><a class="pagination-related" href="/887d60201e7d/" title="markdown基础语法"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-24</div><div class="info-item-2">markdown基础语法</div></div><div class="info-2"><div class="info-item-1">Markdown语法安装离线编辑软件TyporaTypora下载。[官网](https://typoraio.cn/)下载后去github上下载破解器https://github.com/WittonBell/typoraCracker 在线协同编辑器CodiMD在线协同编辑markdown http://120.26.48.253:3000/( 搭建在我的服务器上，所以一年后(2025/3/20)就寄了) 常用语法 中间空出一行(相当于按了两次回车)则新起一段  使用#加一个空格来表示标题   1234# 一号标题## 二号标题### ...###### 六号标题  代码块，使用反引号(在键盘左上角)  一个反引号是段内代码段内代码 ，常用作关键字标记。...</div></div></div></a><a class="pagination-related" href="/b4bdaa4b4566/" title="python配置cuda"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-27</div><div class="info-item-2">python配置cuda</div></div><div class="info-2"><div class="info-item-1">使用pytorch进行深度学习的时候，往往想用GPU进行运算来提高速度。于是搜索便知道了CUDA。 下面给出一个自检的建议：  检查cuda的版本是否适配自己的GPU。  打开NVDIA控制面板，点击左下角“系统信息”，然后就可以看到NVDIA GPU的详细信息，其中就包含了CUDA的版本。在官网安装合适版本的cuda-toolkit。 ![image-20240827162727766](C:\Users\lsk\AppData\Roaming\Typora\typora-user-images\image-20240827162727766.png)  安装了cuda，但是命令行输入nvcc -V 报错显示没有nvcc   这时候可能没有将CUDA添加到环境变量。检查系统变量中是否包含了CUDA_PATH ,以及CUDA_PATH_Vx.x  ,...</div></div></div></a><a class="pagination-related" href="/0a85d83dfe8a/" title="cpp知识点总结"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-27</div><div class="info-item-2">cpp知识点总结</div></div><div class="info-2"><div class="info-item-1">0. 杂类cin/cout提速使用ios::sync_with_stdio(false);加速输入输出速度 **&lt;&lt;的优先级**运算符&lt;&lt;和&gt;&gt; 的优先级比表达式中有的运算符要高，有时候要加上括号 1cout&lt;&lt;(a&lt;b)&lt;&lt;endl; **字符函数库cctype**判断一个字符ch是什么类型的： 1234isalpha(ch);//字母isdight(ch);//数字isspace(ch);//空白（不止是空格）ispunct(ch);//标点 比直接判断ASCII码更加容易使用（有的字符格式没有用ASCII码存就只能这么判断） 文件输入输出IO包含库文件fstream后可以进行文件的读写 创建ofstream对象和ifstream对象来分别对文件进行写和读用法和cout...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comments</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avator.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Little_sk</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">51</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/lsk404/"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">加我友链！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">强化学习数学原理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.1.</span> <span class="toc-text">第一章基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-decision-process%EF%BC%88MDP%EF%BC%89"><span class="toc-number">1.1.1.</span> <span class="toc-text">Markov decision process（MDP）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F"><span class="toc-number">1.2.</span> <span class="toc-text">第二章 贝尔曼公式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#return"><span class="toc-number">1.2.1.</span> <span class="toc-text">return</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#state-value"><span class="toc-number">1.2.2.</span> <span class="toc-text">state value</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F%E7%9A%84%E7%9F%A9%E9%98%B5-%E5%90%91%E9%87%8F%E5%BD%A2%E5%BC%8F"><span class="toc-number">1.2.3.</span> <span class="toc-text">贝尔曼公式的矩阵&#x2F;向量形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#action-value"><span class="toc-number">1.2.4.</span> <span class="toc-text">action value</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F"><span class="toc-number">1.3.</span> <span class="toc-text">第三章 贝尔曼最优公式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F"><span class="toc-number">1.3.1.</span> <span class="toc-text">贝尔曼最优公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5"><span class="toc-number">1.3.2.</span> <span class="toc-text">一些概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#contraction-Theorem"><span class="toc-number">1.3.3.</span> <span class="toc-text">contraction Theorem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%82%E8%A7%A3%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F"><span class="toc-number">1.3.4.</span> <span class="toc-text">求解贝尔曼最优公式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-Value-iteration-amp-Policy-iteration"><span class="toc-number">1.4.</span> <span class="toc-text">第四章 Value iteration &amp; Policy iteration</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Value-iteration-algorithm-%E5%80%BC%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.1.</span> <span class="toc-text">Value iteration algorithm(值迭代算法)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Policy-iteration-algorithm-%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.2.</span> <span class="toc-text">Policy iteration algorithm(策略迭代算法)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#truncated-policy-iteration-algorithm"><span class="toc-number">1.4.3.</span> <span class="toc-text">truncated policy iteration algorithm</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-MonteCarlo-learning"><span class="toc-number">1.5.</span> <span class="toc-text">第五章 MonteCarlo learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%9B%E7%A1%AC%E5%B8%81%E4%BE%8B%E5%AD%90"><span class="toc-number">1.5.1.</span> <span class="toc-text">抛硬币例子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84MC-based-RL%E7%AE%97%E6%B3%95"><span class="toc-number">1.5.2.</span> <span class="toc-text">一个简单的MC-based RL算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MC-exploring-Starts"><span class="toc-number">1.5.3.</span> <span class="toc-text">MC exploring Starts</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#soft-policies"><span class="toc-number">1.5.4.</span> <span class="toc-text">soft policies</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MC-undefinedepsilonundefinedGreedy-algorithm"><span class="toc-number">1.5.5.</span> <span class="toc-text">MC undefinedepsilonundefinedGreedy algorithm</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-Stochastic-Approximation-amp-Stochastic-Grandient-Descent"><span class="toc-number">1.6.</span> <span class="toc-text">第六章 Stochastic Approximation &amp; Stochastic Grandient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Motivating-example"><span class="toc-number">1.6.1.</span> <span class="toc-text">Motivating example</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Robbins-Monro-algorithm"><span class="toc-number">1.6.2.</span> <span class="toc-text">Robbins-Monro algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RM%E7%AE%97%E6%B3%95-Convergence-properties"><span class="toc-number">1.6.3.</span> <span class="toc-text">RM算法-Convergence properties</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD-stochatic-gradient-descent"><span class="toc-number">1.6.4.</span> <span class="toc-text">SGD(stochatic gradient descent)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD-%E7%9A%84%E4%BE%8B%E5%AD%90%E5%92%8C%E7%BB%83%E4%B9%A0"><span class="toc-number">1.6.5.</span> <span class="toc-text">SGD 的例子和练习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B6%E6%95%9B%E6%80%A7-convergence"><span class="toc-number">1.6.6.</span> <span class="toc-text">SGD算法的收敛性(convergence)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B6%E6%95%9B%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.6.7.</span> <span class="toc-text">SGD算法的收敛模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BGD-MBGD%E5%92%8CSGD"><span class="toc-number">1.6.8.</span> <span class="toc-text">BGD,MBGD和SGD</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-Temporal-Difference-learning"><span class="toc-number">1.7.</span> <span class="toc-text">第七章 Temporal-Difference learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#motivating-example"><span class="toc-number">1.7.1.</span> <span class="toc-text">motivating example</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TD%E7%AE%97%E6%B3%95%E4%B8%AD%E7%9A%84state-values"><span class="toc-number">1.7.2.</span> <span class="toc-text">TD算法中的state values</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TD%E7%AE%97%E6%B3%95%E7%9A%84%E6%95%B0%E5%AD%A6%E6%84%8F%E4%B9%89"><span class="toc-number">1.7.3.</span> <span class="toc-text">TD算法的数学意义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TD%E7%AE%97%E6%B3%95%E4%B8%AD%E7%9A%84action-values%EF%BC%9ASarsa"><span class="toc-number">1.7.4.</span> <span class="toc-text">TD算法中的action values：Sarsa</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Expected-Sarsa"><span class="toc-number">1.7.5.</span> <span class="toc-text">Expected Sarsa</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#n-step-Sarsa"><span class="toc-number">1.7.6.</span> <span class="toc-text">n-step Sarsa</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TD%E4%B8%AD%E6%9C%80%E4%BC%98action-value%E5%AD%A6%E4%B9%A0-Q-learning"><span class="toc-number">1.7.7.</span> <span class="toc-text">TD中最优action value学习:Q-learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#off-policy-%E5%92%8C-on-policy"><span class="toc-number">1.7.8.</span> <span class="toc-text">off-policy 和 on-policy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q-learning-%E7%9A%84%E5%AE%9E%E6%96%BD"><span class="toc-number">1.7.9.</span> <span class="toc-text">Q-learning 的实施</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TD%E7%9A%84%E7%BB%9F%E4%B8%80%E8%A1%A8%E7%A4%BA"><span class="toc-number">1.7.10.</span> <span class="toc-text">TD的统一表示</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0-value-function-Approximation"><span class="toc-number">1.8.</span> <span class="toc-text">第八章 value function Approximation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Value-Function-Approximation%E7%9A%84%E5%90%AB%E4%B9%89"><span class="toc-number">1.8.1.</span> <span class="toc-text">Value Function Approximation的含义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#objective-funciton"><span class="toc-number">1.8.2.</span> <span class="toc-text">objective funciton</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#optimization-algorithms"><span class="toc-number">1.8.3.</span> <span class="toc-text">optimization algorithms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#selection-of-function-approximators"><span class="toc-number">1.8.4.</span> <span class="toc-text">selection of function approximators</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sarsa"><span class="toc-number">1.8.5.</span> <span class="toc-text">Sarsa</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q-learning"><span class="toc-number">1.8.6.</span> <span class="toc-text">Q-learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Deep-Q-learning-deep-Q-network"><span class="toc-number">1.8.7.</span> <span class="toc-text">Deep Q-learning(deep Q-network)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DQN-Experience-replay"><span class="toc-number">1.8.8.</span> <span class="toc-text">DQN-Experience replay</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Deep-Q-learning"><span class="toc-number">1.8.9.</span> <span class="toc-text">Deep Q-learning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B9%9D%E7%AB%A0-Policy-Function-Approximation"><span class="toc-number">1.9.</span> <span class="toc-text">第九章 Policy Function Approximation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#policy-gradient%E7%9A%84%E4%B8%A4%E4%B8%AAmetrics"><span class="toc-number">1.9.1.</span> <span class="toc-text">policy gradient的两个metrics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">1.9.2.</span> <span class="toc-text">目标函数的梯度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#REINFORCE"><span class="toc-number">1.9.3.</span> <span class="toc-text">REINFORCE</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E7%AB%A0-actor-critic-%E6%96%B9%E6%B3%95"><span class="toc-number">1.10.</span> <span class="toc-text">第十章 actor-critic 方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-simplest-actor-critic"><span class="toc-number">1.10.1.</span> <span class="toc-text">The simplest actor-critic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#advantage-actor-critic"><span class="toc-number">1.10.2.</span> <span class="toc-text">advantage actor-critic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#off-policy-actor-critic"><span class="toc-number">1.10.3.</span> <span class="toc-text">off-policy actor-critic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Deterministic-actor-critic"><span class="toc-number">1.10.4.</span> <span class="toc-text">Deterministic actor-critic</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/555324d54baf/" title="期末回顾">期末回顾</a><time datetime="2025-06-06T04:00:00.000Z" title="Created 2025-06-06 12:00:00">2025-06-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/887930e5b441/" title="强化学习数学原理笔记">强化学习数学原理笔记</a><time datetime="2025-04-16T09:53:00.000Z" title="Created 2025-04-16 17:53:00">2025-04-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/d49b5a7aa584/" title="2025蓝桥杯PythonA组省赛题解">2025蓝桥杯PythonA组省赛题解</a><time datetime="2025-04-12T12:28:00.000Z" title="Created 2025-04-12 20:28:00">2025-04-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/0ed69ce9ed7c/" title="电子扫盲">电子扫盲</a><time datetime="2025-03-09T04:00:00.000Z" title="Created 2025-03-09 12:00:00">2025-03-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/27d697c28fe4/" title="十六届蓝桥杯模拟赛3题解">十六届蓝桥杯模拟赛3题解</a><time datetime="2025-02-12T04:00:00.000Z" title="Created 2025-02-12 12:00:00">2025-02-12</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(img/bg3.png);"><div id="footer-wrap"><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div><div class="footer_custom_text"><a href="https://icp.gov.moe/?keyword=20250258" target="_blank">萌ICP备20250258号</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll to Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const initValine = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyValine = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const valineConfig = {
      el: '#vcomment',
      appId: 'jY5tnMLwMUhZNOv61yXvepci-gzGzoHsz',
      appKey: 'gZbn0Lr7joLZ83iUlbqPfk4J',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      visitor: false,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || window.location.pathname
    }

    new Valine(valineConfig)
  }

  const loadValine = async (el, path) => {
    if (typeof Valine === 'function') {
      initValine(el, path)
    } else {
      await btf.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js')
      initValine(el, path)
    }
  }

  if (isShuoshuo) {
    'Valine' === 'Valine'
      ? window.shuoshuoComment = { loadComment: loadValine }
      : window.loadOtherComment = loadValine
    return
  }

  if ('Valine' === 'Valine' || !true) {
    if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search what you want to search" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>